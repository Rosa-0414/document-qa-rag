# -*- coding: utf-8 -*-
"""313515035

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wedyjfYh7MdTVxI8fjH9thZ8gDGhD2bE
"""

!pip install transformers accelerate sentence-transformers faiss-cpu datasets bitsandbytes scipy -q

import json
import os

from google.colab import files
uploaded = files.upload()

#file_path = "public_dataset.json"
file_path = "private_dataset.json"

with open(file_path, "r") as f:
    public_data = json.load(f)

print(public_data[0].keys())
print(f"Title: {public_data[0]['title']}")
print(f"Q: {public_data[0]['question']}")
print(f"top100: {public_data[0]['full_text'][:100]}")

"""Chunk"""

def chunk_text(text, chunk_size=300, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

test_text = public_data[0]['full_text']
test_chunks = chunk_text(test_text)
#print(f"{len(test_chunks)}")
#print(f"{test_chunks[0]}")

"""Embedding Model"""

from sentence_transformers import SentenceTransformer
import torch

#BGE-small-en
embed_model_name = "BAAI/bge-small-en"
embed_model = SentenceTransformer(embed_model_name)
embed_model = embed_model.to("cuda" if torch.cuda.is_available() else "cpu")


all_chunks = []
chunk_to_paper = []

for paper in public_data:
    chunks = chunk_text(paper["full_text"], chunk_size=300, overlap=50)
    all_chunks.extend(chunks)
    chunk_to_paper.extend([paper["title"]] * len(chunks))

print(f"{len(all_chunks)}")

#Embedding
chunk_embeddings = embed_model.encode(all_chunks, batch_size=32, show_progress_bar=True, convert_to_tensor=True)

print(f"{chunk_embeddings.shape}")

"""FAISS"""

import faiss
embedding_dim = chunk_embeddings.shape[1]  # 384
index = faiss.IndexFlatIP(embedding_dim)

chunk_embeddings_np = chunk_embeddings.cpu().detach().numpy()
index.add(chunk_embeddings_np)

print(f"{index.ntotal}")

def retrieve_top_k(question, embed_model, index, all_chunks, top_k=5):

    #Embedding
    question_embedding = embed_model.encode([question], convert_to_tensor=True)
    question_embedding_np = question_embedding.cpu().detach().numpy()

    #FAISS
    scores, indices = index.search(question_embedding_np, top_k)

    #retrived chunks
    retrieved_chunks = [all_chunks[idx] for idx in indices[0]]

    return retrieved_chunks, indices[0], scores[0]


sample_question = public_data[0]['question']
retrieved_chunks, retrieved_indices, retrieved_scores = retrieve_top_k(
    sample_question,
    embed_model,
    index,
    all_chunks,
    top_k=5
)

print(sample_question)
for i, chunk in enumerate(retrieved_chunks):
    print(f"\nRank {i+1}:")
    print(f"Score: {retrieved_scores[i]:.4f}")
    print(chunk[:300])

"""Prompt"""

def build_prompt(context_chunks, question):

    context_text = "\n\n".join(context_chunks)

    prompt = f"""You are a helpful assistant specialized in answering questions based on retrieved documents.

Below is the retrieved context information:
---
{context_text}
---

Instructions:
- First, carefully read the context and summarize the key points related to the question.
- Then, based on the summarized information, answer the question clearly and concisely.
- If the context mentions any specific methods (e.g., semi-supervised learning, Cross-View Training (CVT)), please clearly state the method names in your answer.
- Only use the provided context to answer. Do not use any outside knowledge.
- If the answer is not found in the context, reply: "I cannot find the answer from the given context."

Question: {question}

Answer:"""

    return prompt

!pip install openai -q

import os
import openai

os.environ["OPENAI_API_KEY"] = "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"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://api.groq.com/openai/v1"

import openai

client = openai.OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api.groq.com/openai/v1"
)

def ask_llm_groq(prompt, model="llama3-8b-8192"):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant specialized in document question answering."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=512,
    )
    return response.choices[0].message.content.strip()


prompt_v2 = build_prompt(retrieved_chunks, sample_question)
answer = ask_llm_groq(prompt_v2)
print(answer)

import json
import time
from tqdm import tqdm
test_results = []
test_data = public_data[:10]
os.makedirs("outputs", exist_ok=True)

for idx, paper in tqdm(enumerate(test_data), total=len(test_data)):
    try:
        title = paper['title']
        question = paper['question']
        full_text = paper['full_text']

        chunks = chunk_text(full_text, chunk_size=300, overlap=50)
        chunks_embeddings = embed_model.encode(chunks, batch_size=32, convert_to_tensor=True)

        temp_index = faiss.IndexFlatIP(chunks_embeddings.shape[1])
        temp_index.add(chunks_embeddings.cpu().detach().numpy())

        question_embedding = embed_model.encode([question], convert_to_tensor=True)
        question_embedding_np = question_embedding.cpu().detach().numpy()
        top_k = 5
        scores, indices = temp_index.search(question_embedding_np, top_k)
        retrieved_chunks = [chunks[i] for i in indices[0]]


        prompt = build_prompt(retrieved_chunks, question)

        answer = ask_llm_groq(prompt)

        result = {
            "title": title,
            "answer": answer,
            "evidence": retrieved_chunks
        }
        test_results.append(result)

        time.sleep(0.2)

    except Exception as e:
        continue

#submission
with open("outputs/test_submission.json", "w") as f:
    json.dump(test_results, f, indent=2)

print("outputs/test_submission.json is saved.")

with open("outputs/test_submission.json", "r") as f:
    preview = json.load(f)

for item in preview:
    print(f"\nTitle: {item['title']}")
    print(f"Answer: {item['answer'][:200]}...")
    print(f"Evidence: {len(item['evidence'])}")

import json
import time
from tqdm import tqdm

final_results = []
full_data = public_data

os.makedirs("outputs", exist_ok=True)
batch_size = 10

for idx, paper in tqdm(enumerate(full_data), total=len(full_data)):
    try:

        title = paper['title']
        question = paper['question']
        full_text = paper['full_text']

        chunks = chunk_text(full_text, chunk_size=300, overlap=50)
        chunks_embeddings = embed_model.encode(chunks, batch_size=32, convert_to_tensor=True)

        temp_index = faiss.IndexFlatIP(chunks_embeddings.shape[1])
        temp_index.add(chunks_embeddings.cpu().detach().numpy())

        question_embedding = embed_model.encode([question], convert_to_tensor=True)
        question_embedding_np = question_embedding.cpu().detach().numpy()
        top_k = 5
        scores, indices = temp_index.search(question_embedding_np, top_k)
        retrieved_chunks = [chunks[i] for i in indices[0]]

        prompt = build_prompt(retrieved_chunks, question)

        answer = ask_llm_groq(prompt)

        result = {
            "title": title,
            "answer": answer,
            "evidence": retrieved_chunks
        }
        final_results.append(result)

        if (idx + 1) % batch_size == 0:
            with open("outputs/temp_submission.json", "w") as f:
                json.dump(final_results, f, indent=2)

        time.sleep(0.2)

    except Exception as e:
        continue

#submission
with open("outputs/submission.json", "w") as f:
    json.dump(final_results, f, indent=2)

print("outputs/submission.json is saved.")