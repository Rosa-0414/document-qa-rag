[
  {
    "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The dataset consists of scientific articles collected from the ACM Digital Library (conference and workshop papers).\n* The dataset has two parts: training (144 articles) and test (100 articles).\n\nAccording to the summarized information, the answer to the question is:\n\nThere are 244 articles in the dataset (144 in the training set and 100 in the test set).",
    "evidence": [
      "a systematic dehyphenation at line breaks and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphr",
      "r lengths in words, are filtered out. Finding the optimal subset of sentences from already shortened documents is however no trivial task as maximum recall linearly decreases with the number of sentences discarded. Here, we simply set the reduction ratio to 0.865 so that the average maximum recall o",
      "10 benchmark dataset are notoriously difficult to handle due to the large number of keyphrase candidates (i.e. phrases that are eligible to be keyphrases) that the systems have to cope with BIBREF6 . Furthermore, noisy textual content, whether due to format conversion errors or to unusable elements ",
      "Abstract\nThe SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of t",
      "44 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and remo"
    ]
  },
  {
    "title": "Comparative Studies of Detecting Abusive Language on Twitter",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The context-dependent nature of online aggression makes annotating large collections of data extremely difficult.\n* Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models.\n* The authors mention that major social media companies have utilized multiple resources to censor offensive language, but it seems nearly impossible to successfully do so.\n\nThe authors do not give specific examples of the difficulties presented by the context-dependent nature of online aggression. However, they do mention that annotating large collections of data is extremely difficult due to the context-dependent nature of online aggression.\n\nAnswer: The authors do not provide specific examples of the difficulties presented by the context-dependent nature of online aggression.",
    "evidence": [
      "Abstract\nThe context-dependent nature of online aggression makes annotating large collections of data extremely difficult. Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models. Recently, Hate and Abusive Speech on Twitter,",
      "els. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Spee",
      "f various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic Clustering modules, is the most acc",
      "or have witnessed offensive behaviors online BIBREF6 . Major social media companies (i.e. Facebook, Twitter) have utilized multiple resources\u2014artificial intelligence, human reviewers, user reporting processes, etc.\u2014in effort to censor offensive language, yet it seems nearly impossible to successfull",
      "We call these, \u201ccontext tweets\". As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vu"
    ]
  },
  {
    "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context discusses a method for enforcing agreement between parse trees from two languages, specifically English and Hindi.\n* The method involves using parallel data to correct PP-attachments for a sentence in one language (English) based on alignments from the parallel data in another language (Hindi).\n* The algorithm, called Project Algorithm, takes a parse tree from Hindi and a sentence from English as input and tries to bring the parse trees in the two languages into agreement through constraints.\n\nNow, based on the summarized information, here is the answer to the question:\n\nEnforcing agreement between parse trees across different languages works by using parallel data to correct PP-attachments for a sentence in one language based on alignments from the parallel data in another language. The algorithm, called Project Algorithm, takes a parse tree from one language and a sentence from another language as input and tries to bring the parse trees into agreement through constraints. This is achieved by aligning words between the two languages and expecting dependency edges between corresponding words in the parse trees to be similar.",
    "evidence": [
      " enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English.\n\n\nIntroducti",
      "s very crucial. The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from t",
      ", and the word alignments between the parallel sentence. Project Algorithm (tree T, sen S) [1] A parse tree T (Hindi) and sentence S (English) Initialize INLINEFORM0 INLINEFORM1 = 0 for INLINEFORM2 to INLINEFORM3 INLINEFORM4 for INLINEFORM5 INLINEFORM6   INLINEFORM0 end for if INLINEFORM1 then retur",
      "ssentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, and vice ",
      " English edge on the Hindi parse tree, and similarly there are projected paths from Hindi to English. For matters of simplicity, we ignore the direction of the edges in the parse trees. The dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constra"
    ]
  },
  {
    "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* NeuronBlocks is a toolkit that encapsulates a suite of neural network modules as building blocks to construct various DNN models with complex architecture.\n* The toolkit is built on PyTorch and consists of two layers: the Block Zoo and the Model Zoo.\n* The Block Zoo categorizes the most commonly used components of deep neural networks into several groups.\n\nAccording to the context, the answer to the question is:\n\nThe neural network modules included in NeuronBlocks are categorized into several groups in the Block Zoo, but the specific modules are not explicitly listed in the provided context.",
    "evidence": [
      "ions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple configura",
      "results suggest that the flexible combination of layers/blocks in NeuronBlocks can easily reproduce the performance of original models, with comparative or slightly better performance.\n\n\nGLUE Benchmark\nThe General Language Understanding Evaluation (GLUE) benchmark BIBREF13 is a collection of natural",
      "ith simple model configurations.\n\n\nConclusion and Future Work\nIn this paper, we introduce NeuronBlocks, a DNN toolkit for NLP tasks built on PyTorch. NeuronBlocks targets three types of engineers, and provides a two-layer solution to satisfy the requirements from all three types of users. To be more",
      " can simply browse these configurations and choose one or more to instantiate. The whole task can be completed without any coding efforts. \n\n\nBlock Zoo\n We recognize the following major functional categories of neural network components. Each category covers as many commonly used modules as possible",
      "tures or new components. \n\n\nDesign\n The Neuronblocks is built on PyTorch. The overall framework is illustrated in Figure FIGREF16 . It consists of two layers: the Block Zoo and the Model Zoo. In Block Zoo, the most commonly used components of deep neural networks are categorized into several groups "
    ]
  },
  {
    "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The authors evaluated 19 different algorithms to recommend tags for annotating e-books.\n* The algorithms used are categorized into two types: popularity-based approaches and similarity-based approaches.\n* The authors combined the best three performing popularity-based approaches to create a popularity-based hybrid algorithm.\n* The authors also compared their proposed measure to standard accuracy measures such as INLINEFORM0, using Kendall's Tau rank correlation.\n\nAs for the answer to the question, the authors used the following algorithms:\n\n* Popularity-based approaches: The authors recommend the most frequently used tags in the dataset, which is a combination of three popularity-based approaches.\n* Similarity-based approaches: The authors used the best performing similarity-based approach, but the specific algorithm is not mentioned in the provided context.\n* Other algorithms: The authors did not implement other types of algorithms such as collaborative filtering.\n\nNote that the specific similarity-based algorithm used is not mentioned in the provided context, so I cannot provide more information on that.",
    "evidence": [
      "1 shows the accuracy results of the four hybrid approaches. By combining the best three popularity-based approaches, we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ). On the contrary, the combination of the two best performing similarity-based a",
      " that uses the best performing popularity-based and the best performing similarity-based approach (see Figure FIGREF11 in Section SECREF4 for more details about the particular algorithm combinations).\n\n\nExperimental Setup\nIn this section, we describe our evaluation protocol as well as the measures w",
      "s. In this case, tags are favored that are recommended by more than one algorithm. Hence, to create a popularity-based hybrid (= INLINEFORM0 ), we combine the best three performing popularity-based approaches from the ones (i) without any contextual signal, (ii) with the author as context, and (iii)",
      "imilarity measure. To compare our proposed measure to standard accuracy measures such as INLINEFORM0 , we use Kendall's Tau rank correlation BIBREF19 as suggested by BIBREF20 for automatic evaluation of information-ordering tasks. From that, we rank our recommendation approaches according to both ac",
      " has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8 . In total, we evaluate 19 different algorithms to recommend tags for annotating e-books.  Popularity-based approaches. We recommend the most frequently used tags in the dataset, which is a co"
    ]
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection",
    "answer": "Based on the provided context, the best reported system is the one that uses the extended feature-set, which includes gaze-based features and complex features derived from a graph structure. This system improved the success rate of the sarcasm detector by 3.7% over the best available system.",
    "evidence": [
      "pport. \n\n\n",
      "lassifiers and other systems. These are: Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along w",
      "detection may be a cost-effective solution. To examine this, we repeated our experiments with \u201creading time\u201d considered as the only cognitive feature, augmented with the textual features. The F-scores of all the classifiers turn out to be close to that of the classifiers considering sarcasm feature ",
      "ed from readers' eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure. This extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system. Using cognitive features in an NLP Processing system ",
      "aze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall. To see if the improvement obtained is statistically significant over the state-of-the art system with textual sarcasm features alone, we perform McNemar test. The output o"
    ]
  },
  {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "answer": "Based on the provided context, the offensive dataset is not biased by topic, dialect, or target because the methodology used does not rely on a seed list of offensive words. This approach ensures that the dataset is not influenced by preconceived notions or assumptions about what constitutes offensive language.\n\nIn other words, the dataset is not biased because it is not based on a specific set of words or phrases that are deemed offensive, but rather it is built using a more comprehensive and nuanced approach that takes into account the context and nuances of Arabic language.",
    "evidence": [
      "lding an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Ar",
      "e our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate spe",
      "Abstract\nDetecting offensive language on Twitter has many applications ranging from detecting/predicting bullying to measuring polarization. In this paper, we focus on building effective Arabic offensive tweet detection. We introduce a method for building an offensive dataset that is not biased by t",
      "ngs produced a competitive results on the dataset. For future work, we plan to pursue several directions. First, we want explore target specific offensive language, where attacks against an entity or a group may employ certain expressions that are only offensive within the context of that target and",
      ". Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that ar"
    ]
  },
  {
    "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The context discusses a Bayesian model for semantic role induction, specifically the monolingual model.\n* The monolingual model is based on the work of Garg et al. (2012).\n\nAccording to the context, an individual model (in this case, the monolingual model) consists of:\n\n* A Bayesian model that uses a predicate-specific role inventory to model role ordering and repetition preferences.\n\nIn other words, the individual model is a Bayesian model that represents the semantic roles associated with each predicate and models the preferences for role ordering and repetition.",
    "evidence": [
      "osition of the monolingual model in Figure FIGREF7 . However, having this additional conditioning variable breaks the Dirichlet-multinomial conjugacy, which makes it intractable to marginalize out the parameters during inference. Hence, we use an approximation where we treat each of the aligned role",
      "rs are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:  DISPLAYFORM0   where the subscript INLINEFORM0 refers to all the variables except at position INLINEFO",
      "posed a Bayesian model for this problem that we use here. titov2012bayesian also proposed a closely related Bayesian model. grenager2006unsupervised proposed a generative model but their parameter space consisted of all possible linkings of syntactic constituents and semantic roles, which made unsup",
      "obtained in different settings involving labeled and unlabeled data, and analyzing the annotation effort required to obtain similar gains using labeled data. We begin by briefly describing the unsupervised SRL pipeline and the monolingual semantic role induction model we use, and then describe our m",
      "ervised semantic role induction, is the step 4 of this pipeline.\n\n\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicat"
    ]
  },
  {
    "title": "Hierarchical Transformers for Long Document Classification",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The text mentions the use of a recurrent layer or another transformer on top of the BERT model.\n* It is mentioned that the features extracted from a pre-trained BERT model without fine-tuning lead to a sub-par performance.\n* The text also mentions that ToBERT model exploited the pre-trained BERT features better than RoBERT.\n\nBased on this information, I can answer the question as follows:\n\nAccording to the context, the text does not explicitly state which layer (RNN or transformer) works better on top of BERT. However, it does mention that the features extracted from a pre-trained BERT model without fine-tuning lead to a sub-par performance, and that ToBERT model exploited the pre-trained BERT features better than RoBERT. This suggests that the transformer layer may be more effective, but the text does not provide a direct comparison between the two layers.",
    "evidence": [
      "inal transformer block as these were shown to be working well for most of the tasks BIBREF1. The features extracted from a pre-trained BERT model without any fine-tuning lead to a sub-par performance. However, We also notice that ToBERT model exploited the pre-trained BERT features better than RoBER",
      "ropagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of t",
      "BERT, the Transformer is trained with the default BERT version of Adam optimizer BIBREF1 with an initial learning rate of $5e$-5. We report accuracy in all of our experiments. We chose a model with the best validation accuracy to calculate accuracy on the test set. To accomodate for non-determinism ",
      "Abstract\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm. We extend its fine-tuning procedure to address one of its major limitations - applicability to inputs longer than",
      "newsgroups and Fisher. We observed that ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks. Also, we noticed that fine-tuned BERT performs better than pre-trained BERT. We have shown that both RoBERT and ToBERT improved the simple baselines of takin"
    ]
  },
  {
    "title": "Impact of Batch Size on Stopping Active Learning for Text Classification",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The context discusses the performance of stopping methods in an active learning process.\n* The stopping methods are used to decide when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking for additional labels during each iteration.\n* The context mentions that stopping methods degrade in performance when larger batch sizes are used, but decreasing the window size parameter can help to reduce this degradation.\n\nThe context does not explicitly mention the downstream tasks being evaluated. Therefore, I cannot find the answer from the given context.\n\nAnswer: I cannot find the answer from the given context.",
    "evidence": [
      "erformance Computing System.\n\n\n",
      "n is whether changing the parameters associated with actual stopping methods can make them experience less degradation in performance when larger batch sizes are used. In particular, an important parameter of stopping methods is the window size of previous iterations to consider. The next subsection",
      "active learning process are when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking for additional labels during each iteration. We found that stopping methods degrade in performance when larger batch sizes are used. The degradation in perf",
      "evious iterations to consider. The next subsection shows how decreasing the window size parameter can help to reduce the degradation in performance that stopping methods experience with larger batch sizes.\n\n\nComparing BV2009 with the Oracle Method\nWe denote the stopping method published in BIBREF4 a",
      "aken into account when making the stopping decision. We find that when using larger batch sizes, stopping methods are more effective when smaller window sizes are used.\n\n\nIntroduction\nThe use of active learning has received a lot of interest for reducing annotation costs for text classification BIBR"
    ]
  },
  {
    "title": "Generating Personalized Recipes from Historical User Preferences",
    "answer": "Based on the provided context, it can be summarized that the recipes are generated from user preferences extracted from previously consumed recipes. The model's input specification consists of the recipe name, a partial list of ingredients, and a caloric level, which are used to generate recipe content.\n\nAnswer: The recipes are generated from user preferences extracted from previously consumed recipes.",
    "evidence": [
      "sider it for the problem of recipe generation, where output quality is heavily dependent on the content of the instructions\u2014such as ingredients and cooking techniques. To summarize, our main contributions are as follows: We explore a new task of generating plausible and personalized recipes from inc",
      "Abstract\nExisting approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of ingredients in specific dishes. We propose a new task of personalized recipe generation to help these users: expanding a name and incomplete ingredient detai",
      "rence (e.g. culinary tastes, garnish choices) and incomplete recipe details. We propose to approach both problems via personalized generation of plausible, user-specific recipes using user preferences extracted from previously consumed recipes. Our work combines two important tasks from natural lang",
      "re, we attend over historical user preferences from previously consumed recipes to generate recipe content, rather than writing styles.\n\n\nApproach\nOur model's input specification consists of: the recipe name as a sequence of tokens, a partial list of ingredients, and a caloric level (high, medium, l",
      " The order of recipe presentation (A/B) is randomly selected for each question. A screenshot of the user evaluation interface is given in fig:exeval. We ask the user to indicate which recipe they find more coherent, and which recipe best accomplishes the goal indicated by the recipe name. A screensh"
    ]
  },
  {
    "title": "Pyramidal Recurrent Unit for Language Modeling",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors use the PTB (Penn Treebank) dataset and the WT-2 dataset for their experiments.\n* The PTB dataset is mentioned as having a specific range of parameters learned by state-of-the-art methods (18M to 66M, with most methods learning around 22M to 24M parameters).\n* The WT-2 dataset is mentioned as having a specific improvement in RUs (about 7 points) and perplexity (about 1 point) when using the PRU model.\n\nBased on these key points, the answer to the question is:\n\nThe authors used the PTB and WT-2 datasets for their experiments.",
    "evidence": [
      "1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section SECREF39 ). We mea",
      "th an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of",
      "RUs improves by about 4 points on the PTB dataset and 7 points on the WT-2 dataset. This further improves with finetuning on the PTB (about 2 points) and WT-2 (about 1 point) datasets. For similar number of parameters, the PRU with standard dropout outperforms most of the state-of-the-art methods by",
      " dataset and by about 3 points on the WT-2 dataset. This is explained in part by the regularization effect of the grouped linear transformation (Figure FIGREF1 ). With grouped linear and pyramidal transformations, PRUs learn rich representations at very high dimensional space while learning fewer pa",
      "rs improves the perplexity by about 1 point on both the PTB and WT-2 datasets while reducing the number of recurrent unit parameters by about 14% (see R1 and R4). We note that the performance of the PRU drops by up to 1 point when residual connections are not used (R4 and R6). (2) Using the grouped "
    ]
  },
  {
    "title": "Emotion Detection in Text: Focusing on Latent Representation",
    "answer": "Based on the provided context, the GRU model captures more meaningful information from text, which traditional ML models do not. This is because the GRU model is designed to capture the sequential nature of the text data, whereas traditional ML models may not be able to effectively capture this information. The GRU model uses a bidirectional approach, analyzing both the past and future context of the text, which allows it to capture more nuanced and context-dependent information.",
    "evidence": [
      "ate an intermediate representation for the tweets that capture the sequential nature of the data. For the next step, we use a concatenation of global max-pooling and average-pooling layers (with a window size of two). Then a max-pooling was used to extract the most important features form the GRU ou",
      "and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improveme",
      "xtract the most important features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole. These partial representations are then were concatenated to create out final hidden representation. For classification, the outpu",
      "rm well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets. For buil",
      "emotions compare to results reported by Wang et al.\n\n\nModel Performances on New Dataset\nTo asses the performance of these models on a totally unseen data, we tried to classify the CrowdFlower emotional tweets dataset. The CrowdFlower dataset consists of 40k tweets annotated via crowd-sourcing each w"
    ]
  },
  {
    "title": "ReviewQA: a relational aspect-based opinion reading dataset",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The hotel reviews used in the dataset were extracted from TripAdvisor.\n* The reviews were originally proposed in two articles, BIBREF10 and BIBREF11.\n\nAccording to the summarized information, the hotel reviews are from TripAdvisor.\n\nAnswer: The hotel reviews are from TripAdvisor.",
    "evidence": [
      "t http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value, room, location, cleanliness, check-in/front desk, service, b",
      "iginally proposed in BIBREF10 , BIBREF11 . In the original data, each review comes with a set of rated aspects among the seventh available: Business service, Check in / Front Desk, Cleanliness, Location, Room, Sleep Quality, Value and for all the reviews an Overall rating. In this articles we propos",
      "n this comment ? is not an easy question that can be answered without a deep understanding of the review. It is necessary to capture all the aspects mentioned in the text, to predict their rating and finally to select the best one. The tasks and the dataset we propose are publicly available at http:",
      "ant information extracted from a document. To set up this challenge, we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . In the ",
      "nt in order to produce an answer when it is required. To motivate this purpose, we present ReviewQA, a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that we expect a model to master. Indeed, each question"
    ]
  },
  {
    "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata",
    "answer": "Based on the provided context, it can be summarized that the authors combine a deep learning model with a knowledge base by linking entities from the deep learning method to the related entities from the knowledge base. Specifically, they use Wikidata as the knowledge base and integrate symbolic information into the reasoning process of the neural network model.\n\nAnswer: The authors combine a deep learning model with a knowledge base by linking entities from the deep learning method to the related entities from the knowledge base, specifically using Wikidata as the knowledge base.",
    "evidence": [
      "as two separate tasks, whereas we try to combine the two. It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redir",
      "e are a number of knowledge bases that provide a background repository for entity classification of this type. For this study, we use Wikidata, which can be seen diagrammatically in Figure FIGREF12 . Systems such as DeepType BIBREF25 integrate symbolic information into the reasoning process of a neu",
      "urthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-vali",
      " neural network model which used long short-term memory (LSTMs) to encode the context of the entity, then used an attention mechanism to allow the model to focus on relevant expressions in the entity mention's context. To learn entity representations, we propose a scheme which is potentially more ge",
      "xception of time (65%)\n\n\nConclusion and Future Work\nIn this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates t"
    ]
  },
  {
    "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The authors define a concept map as a labeled graph showing concepts as nodes and relationships between them.\n\nAccording to the context, the authors define a concept map as follows:\n\n\"A concept map is defined as a labeled graph showing concepts as nodes and relationships between them.\"\n\nAnswer: A concept map is defined as a labeled graph showing concepts as nodes and relationships between them.",
    "evidence": [
      "Abstract\nConcept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task a",
      "eterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. For concept map generation, one corpus with human-created summary concept maps for student essays has been created BIBREF28 . In contrast to our corpus, it only deals with single documents, requires a two order",
      "ask\nConcept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts as nodes and relationships between them a",
      "cted 30 of the topics for which we created the necessary concept map annotations.\n\n\nProposition Extraction\nAs concept maps consist of propositions expressing the relation between concepts (see Figure FIGREF2 ), we need to impose such a structure upon the plain text in the document clusters. This cou",
      "ever, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficien"
    ]
  },
  {
    "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus",
    "answer": "Based on the provided context, the quality of the data is empirically evaluated in the following ways:\n\n* Manual inspection of translations with low BLEU scores to ensure they meet the required quality standards.\n* Sanity checks on the overlaps of train, development, and test sets to ensure they are disjoint.\n* Measurement of perplexity of translations using a language model trained on a large amount of clean data.\n\nThese methods are used to evaluate the quality of the data and ensure that it meets the required standards.",
    "evidence": [
      " | g\\in G, |g|>1, \\textrm {Mean}(g) > 0 \\rbrace $. $\\textrm {BLEU}_{MS}$ provides a normalized quality score as oppose to corpus-level BLEU or unnormalized average of sentence BLEU. And $\\textrm {CoefVar}_{MS}$ is a standardized measure of model stability against different speakers (the lower the be",
      "a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually inspected examples where the source transcript was identical to the translation. 3) We measured the perplexity of the translations using a language model trained on a large amount of clean m",
      "ce embeddings BIBREF17. Samples with low scores were manually inspected and sent back for translation when needed. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.\n\n\nData Colle",
      "by multiple speakers with different genders, accents and age groups. Besides the standard corpus-level BLEU scores, we also want to evaluate model output variance on the same content (transcript) but different speakers. We hence propose to group samples (and their sentence BLEU scores) by transcript",
      " describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free t"
    ]
  },
  {
    "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation.\n* A collection of samples of about 100 sentences were taken from the test set results for evaluation.\n* The annotators were instructed to evaluate the translation results on a 5-point scale for fluency and adequacy.\n\nBased on this information, here is the answer to the question:\n\nThe human judgements were assembled by having a group of 50 native speakers evaluate a collection of 100 sentences from the test set results on a 5-point scale for fluency and adequacy.",
    "evidence": [
      "fective where they made a distinction between global and local attention by means of AER scores of the attention vectors. A Gaussian distribution and a monotonic lookup were used to facilitate the corresponding local source sentence look-up.\n\n\nConclusion\nThus, it is seen that the use of morphologica",
      "ng values (between RNNSearch and RNNMorph outputs) were estimated in Table TABREF30 . A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for c",
      "cketing\nThe input source and target language sentences used for training were taken and divided into bucketed pairs of sentences of a fixed number of sizes. This relationship was determined by examining the distribution of words in the corpus primarily to minimize the number of PAD tokens in the sen",
      "ntences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much",
      "r, a weighted sum of the inputs is given for the decoder. This enabled selective lookup to the source sentence during decoding and is known as the attention mechanism BIBREF27 . The attention mechanism was further analysed by Luong et al. luong2015effective where they made a distinction between glob"
    ]
  },
  {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The dataset of 12,594 unique English hashtags is sourced from the Stanford dataset.\n* The dataset includes all 12,594 unique English hashtags and their associated tweets.\n\nAnswer: The dataset of hashtags is sourced from the Stanford dataset, which includes all 12,594 unique English hashtags and their associated tweets.",
    "evidence": [
      "aightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by",
      "s trained on web data. In this paper, we reimplemented and tailored this approach to segmenting hashtags by using a language model specifically trained on Twitter data (implementation details in \u00a7 SECREF26 ). The performance of this method itself is competitive with state-of-the-art methods (evaluat",
      "dditional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.\n\n\nExperiments\nIn this section, we present experimental results that compare our proposed method with the oth",
      "Abstract\nHashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc co",
      "ting a hashtag into a meaningful word sequence. Our contributions are: Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags BIBREF10 . We frame the segmentation ta"
    ]
  },
  {
    "title": "Extractive Summarization of EHR Discharge Notes",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context does not explicitly mention the specific datasets used.\n* However, it mentions a small dataset with known heterogeneity among annotators, which was used to evaluate the model's performance.\n\nAnswer: The context does not provide specific information about the datasets used. However, it mentions a small dataset with known heterogeneity among annotators, which was used to evaluate the model's performance.",
    "evidence": [
      "on allows for comparing medically relevant concepts, cTAKES's CUI labelling process is not perfect, and further work, perhaps through sophisticated regular expressions, is needed to define the limits of extractive summarization.\n\n\nLabeling History of Present Illness Notes\nWe developed a classifier t",
      "ound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be a",
      "uch a classifier can be used to create topic specific evaluation sets for methods that perform extractive summarization. These aims are critical steps in ultimately developing methods that can automate discharge summary creation.\n\n\nRelated Work\nIn the broader field of summarization, automization was",
      "ch batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs. We evaluated the impact of character embeddings, the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report perfor",
      "rrors. Here we provide an upper bound on extractive summarization of discharge notes and develop an LSTM model to sequentially label topics of history of present illness notes. We achieve an F1 score of 0.876, which indicates that this model can be employed to create a dataset for evaluation of extr"
    ]
  },
  {
    "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The participants were instructed to read sentences naturally, without any specific task other than comprehension.\n* The sentences were selected from the Wikipedia corpus provided by culotta2006integrating.\n* The participants read 739 sentences during the recording session.\n\nAnswer: The participants read sentences from the Wikipedia corpus, specifically 739 sentences, which were selected from the corpus provided by culotta2006integrating.",
    "evidence": [
      "ocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences.\n\n\nCorpus Construction ::: Experimental design ::: Normal reading (NR)\nIn the first task, participants were instructed to read the sentences naturally, without any",
      "ucted to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an example sentence as it was depicted on the screen during recording. As shown in Figure FIG",
      "nd calibrate the devices, and the personal reading speed of the participants. We recorded 14 blocks of approx. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for",
      " Moreover, we also report the scores the participants achieved with their answers to the reading comprehension control questions and their relation annotations. The detailed scores for all participants are also presented in Table TABREF4.\n\n\nCorpus Construction ::: Data acquisition\nData acquisition t",
      "ction ::: Reading materials\nDuring the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. This corpus was chosen because it provides annotations of semantic relations. We included seven of the originally defined rel"
    ]
  },
  {
    "title": "Data Collection for Interactive Learning through the Dialog",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The dataset was collected with 1900 dialogs and 8533 turns.\n* The topics discussed in the dialogs were questions randomly chosen from the training examples of the Simple questions dataset.\n* The dataset was collected to enable interactive learning and potentially evaluate how different systems perform on this task.\n\nAccording to the context, the data was collected through a process described in the \"Dataset Collection Process\" section. The process involved collecting dialogs containing all information used for interactive learning and omitting parts that were not requested by the policy.\n\nAnswer: The data was collected through a process that involved collecting dialogs containing all information used for interactive learning and omitting parts that were not requested by the policy.",
    "evidence": [
      "uns of the dataset collection process (google it, check wikipedia, I would need... $\\rightarrow $ Negate).\n\n\nDataset Properties\nWe collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 data",
      "hen it was evident to them that the answer points to the same Freebase entity that was present in Simple questions dataset for that particular question. However, a large amount of questions from that dataset is quite general - with many possible answers. Therefore lot of answers from users were labe",
      "ring the data collection. We believe that with conversation about specific aspects of the discussed question it will be possible to gain even more interesting information from users. The other area of our interest is in possibilities to improve question answering accuracy on test questions of Simple",
      "y studied for the purposes of information retrieval BIBREF5 , BIBREF6 . The main purpose of the collected dataset is to enable interactive learning using the steps proposed above and potentially to evaluate how different systems perform on this task.\n\n\nDialog policies\nThe obvious difficulty when dev",
      " dialog. Therefore, we can collect dialogs containing all information used for interactive learning and omit those parts that were not requested by the policy. We collected the dataset (see Section \"Dataset Collection Process\" ) that enables simulation where the policy can decide how much extra info"
    ]
  },
  {
    "title": "Distilling Translations with Visual Awareness",
    "answer": "Based on the provided context, the approach achieves state-of-the-art performance on the English-German dataset for transformer networks (constrained models) as compared to BIBREF30.",
    "evidence": [
      "g the Multi30K dataset as is (standard setup), we probe the ability of our models to address the three linguistic phenomena where additional context has been proved important (Section ): ambiguities, gender-neutral words and noisy input. In a controlled experiment where we aim to remove the influenc",
      "image information. We first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test ",
      "ce words. The procedure is applied to the train, validation and test sets. For the resulting dataset generated for each setting, we compare models having access to text-only context versus additional text and multimodal contexts. We seek to get insights into the contribution of each type of context ",
      " proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target. Our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is r",
      "th the BLANK token. The statistics of the resulting datasets for the three degradation strategies are shown in Table TABREF10 . We note that RND and PERS are the same for language pairs as the degradation only depends on the source side, while for AMB the words replaced depend on the target language"
    ]
  },
  {
    "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The framework was experimented on two parallel corpora: English-Japanese and Japanese-English translation tasks.\n* The two parallel corpora used are:\n\t+ Asian Scientific Paper Excerpt Corpus (ASPEC)\n\t+ NTCIR PatentMT Parallel Corpus\n\nAccording to the context, the answer to the question is:\n\nThey used the Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.",
    "evidence": [
      "f the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor. Our experiments show that their method offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the ",
      "llel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity. Japanese sentences were segmented by the morphological analyzer M",
      "s of PBSMT to the input of NMT. Mi2016a and Feng2016 introduced a distributed version of coverage vector taken from PBSMT to consider which words have been already translated. All these methods, including ours, employ information of the source sentence to improve the quality of translation, but our ",
      " this paper, however, we do not use a beam search for simplicity and effectiveness.\n\n\nExperiments\nWe evaluated the encoder-decoder-reconstructor framework for NMT on English-Japanese and Japanese-English translation tasks.\n\n\nDatasets\nWe used two parallel corpora: Asian Scientific Paper Excerpt Corpu",
      "ces were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus. Note that sentences with more than 40 words were excluded from the training data.\n\n\nModel"
    ]
  },
  {
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "answer": "Based on the provided context, the biases found in the dataset are:\n\n* Stereotype-driven descriptions\n* Unwarranted inferences\n* Biases and unwarranted inferences that can be found in the Flickr30K dataset\n\nThese biases can lead to language models trained on this data propagating harmful stereotypes, such as the idea that women are less suited for leadership positions.",
    "evidence": [
      "paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applicati",
      "nces resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases.\n\n\n",
      "be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions. This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I ",
      "e speculative in nature, which they say hurts the accuracy and the consistency of the descriptions. But the problem is not with the lack of consistency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased. And so language models tra",
      " helpful by providing a detailed description. In section 3 I will discuss how to establish whether or not there is any bias in the data regarding the use of adjectives.\n\n\nUnwarranted inferences\nUnwarranted inferences are statements about the subject(s) of an image that go beyond what the visual data"
    ]
  },
  {
    "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    "answer": "Based on the provided context, the size of the dataset is 23,700 queries, which includes 22,500 in-scope queries covering 150 intents and 1,200 out-of-scope queries.",
    "evidence": [
      "cted, 100 are used for validation and 100 are used for training, leaving 1,000 for testing.\n\n\nDataset ::: Data Preprocessing and Partitioning\nFor all queries collected, all tokens were down-cased, and all end-of-sentence punctuation was removed. Additionally, all duplicate queries were removed and r",
      "om/clinc/oos-eval.\n\n\nDataset\nWe introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.\n\n\nDataset ::: ",
      "e are intended to represent production scenarios where data may be in limited or uneven supply.\n\n\nBenchmark Evaluation\nTo quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.\n\n\nBenchmark Evaluation ::",
      "ere we augment the 250 OOS+ out-of-scope training queries with 14,750 sentences sampled from Wikipedia. From a development point of view, the oos-train and oos-binary methods both require careful curation of an out-of-scope training set, and this set can be tailored to individual systems. The oos-th",
      "re short and unstructured, in the same style made by real users of task-oriented systems. The queries cover 150 intents, plus out-of-scope queries that do not fall within any of the 150 in-scope intents. We evaluate a range of benchmark classifiers and out-of-scope handling methods on our dataset. B"
    ]
  },
  {
    "title": "Automatic Judgment Prediction via Legal Reading Comprehension",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The authors decide to build a new dataset for evaluation since none of the previous datasets have been published.\n* The new dataset is constructed by randomly collecting cases from China Judgments Online.\n* The dataset is divided into three parts: training (INLINEFORM1 cases), validation (INLINEFORM2 cases), and testing (INLINEFORM2 cases).\n\nAccording to the summary, the dataset used in the experiment is the newly constructed dataset from China Judgments Online, which is divided into training, validation, and testing sets.",
    "evidence": [
      "onstruct a real-world dataset for experiments, and plan to publish it for further research. (3) Besides baselines from previous works, we also carry out comprehensive experiments comparing different existing deep neural network methods on our dataset. Supported by these experiments, improvements ach",
      "ngs often come with several kinds of pleas, e.g. seeking divorce, custody of children, compensation, and maintenance, which focuses on different aspects and thus makes it a challenge for judgment prediction.\n\n\nDataset Construction for Evaluation\nSince none of the datasets from previous works have be",
      "les and their corresponding juridical interpretations. We also implement and evaluate previous methods on our dataset, which prove to be strong baselines. Our experiment results show significant improvements over previous methods. Further experiments demonstrate that our model also achieves consider",
      "e none of the datasets from previous works have been published, we decide to build a new one. We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing. Among the original cases, INLINEFORM3 are gran",
      "ws are set to 1, 3, 4, and 5 with each filter containing 200 feature maps. We add a dropout layer BIBREF38 after the CNN layer with a dropout rate of INLINEFORM1 . We use Adam BIBREF32 for training and set learning rate to INLINEFORM2 , INLINEFORM3 to INLINEFORM4 , INLINEFORM5 to INLINEFORM6 , INLIN"
    ]
  },
  {
    "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context discusses Knowledge Base Question Answering (KBQA) systems, which answer questions by obtaining information from Knowledge Base (KB) tuples.\n* The core component of the KBQA system is not explicitly mentioned, but it is mentioned that the system performs two key tasks: entity linking and relation detection.\n\nAnswer: The core component of the KBQA system is not explicitly mentioned in the provided context. However, it can be inferred that the system's ability to perform entity linking and relation detection is crucial for its functioning.",
    "evidence": [
      " single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.\n\n\nIntroduction\nKnowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . For an input question, these systems typi",
      "i) We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks.\n\n\nBackground: Different Granularity in KB Relations\nPrevious research BIBREF4 , BIBREF20 formulates KB relation detection as",
      " these two baselines are specially designed/tuned for one particular dataset, they do not generalize well when applied to the other dataset. In order to highlight the effect of different relation detection models on the KBQA end-task, we also implemented another baseline that uses our KBQA system bu",
      ", which can be answered with a single $<$ head-entity, relation, tail-entity $>$ KB tuple BIBREF6 , BIBREF7 , BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linki",
      "rms the previous methods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For future work, we will investigate the integration of our HR-BiLSTM into end-to-end systems. For example, our model could be integrated into the decoder in BIBREF31 , to provide better "
    ]
  },
  {
    "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The paper does not explicitly mention the specific datasets used for evaluation.\n* The paper does mention evaluation metrics, such as Mean Rank and Hits@p, but does not specify the datasets used to calculate these metrics.\n\nTherefore, I cannot find the answer to the question from the given context.",
    "evidence": [
      "sing, i.e., predict $t$ given $(h, r)$ or predict $h$ given $(r, t)$ . Rather than requiring one best answer, this task emphasizes more on ranking a set of candidate entities from the knowledge graph. Similar to BIBREF2 , we use two measures as our evaluation metrics. (1) Mean Rank: the averaged ran",
      " $ among $\\lbrace 1, 2\\rbrace $ , the embedding dimension $d$ among $\\lbrace 20, 50, 100\\rbrace $ , the regularization $\\eta $ among $\\lbrace 0, 1E{-5}, 1E{-6}\\rbrace $ , two learning rates $\\lambda _s$ and $\\lambda _t$ among $\\lbrace 0.001, 0.01, 0.05\\rbrace $ to learn the parameters of structure a",
      "diction and triplet classification show that our joint models can handle the sparsity problem well and outperform the baseline method on all metrics with a large margin. Our contributions in this paper are summarized as follows.\n\n\nKnowledge Graph Embedding\nIn this section, we briefly introduce the b",
      " and a higher Hits@p should be achieved by a good embedding model. We call this evaluation setting \u201cRaw\u201d. Since a false predicted triplet may also exist in knowledge graphs, it should be regard as a valid triplet. Hence, we should remove the false predicted triplets included in training, validation ",
      " _{i=1}^{n} \\alpha _i(r) * \\mathbf {z}_i.$$   (Eq. 13) \n\n\nJoint Structure and Text Encoder\nSince both the structure and text description provide valuable information for an entity , we wish to integrate all these information into a joint representation. We propose a united model to learn a joint rep"
    ]
  },
  {
    "title": "State-of-the-Art Vietnamese Word Segmentation",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The paper reviews state-of-the-art approaches and systems for word segmentation in Vietnamese.\n* The study discusses building the corpus stage, approaches applied to solve the word segmentation, and existing toolkits to segment words in Vietnamese sentences.\n\nAccording to the context, the approaches applied to solve word segmentation in Vietnamese are not explicitly mentioned. However, it is mentioned that the study discusses building the corpus stage and implementing machine learning techniques to improve the accuracy for Vietnamese word segmentation.\n\nTherefore, the answer to the question is:\n\n\"I cannot find the answer from the given context.\"",
    "evidence": [
      " paper aims reviewing state-of-the-art word segmentation approaches and systems applying for Vietnamese. This study will be a foundation for studies on Vietnamese word segmentation and other following Vietnamese tasks as well, such as part-of-speech tagger, chunker, or parser systems. There are seve",
      "Abstract\nWord segmentation is the first step of any tasks in Vietnamese language processing. This paper reviews stateof-the-art approaches and systems for word segmentation in Vietnamese. To have an overview of all stages from building corpora to developing toolkits, we discuss building the corpus s",
      "lementing machine learning techniques to improve the accuracy for Vietnamese word segmentation. According to our observation, this study also reports a few of achivements and limitations in existing Vietnamese word segmentation systems.\n\n\nIntroduction\nLexical analysis, syntactic analysis, semantic a",
      "eloping toolkits, we discuss building the corpus stage, approaches applied to solve the word segmentation and existing toolkits to segment words in Vietnamese sentences. In addition, this study shows clearly the motivations on building corpus and implementing machine learning techniques to improve t",
      "esearch cited vnTokenizer to use word segmentation results for applications as building a large Vietnamese corpus BIBREF31 , building an English-Vietnamese Bilingual Corpus for Machine Translation BIBREF32 , Vietnamese text classification BIBREF33 , BIBREF34 , etc.\n\n\nEVALUATION AND RESULTS\nThis rese"
    ]
  },
  {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The text mentions a baseline transformer model, referred to as \"baseline transformer BIBREF8\".\n* The baseline transformer is compared to the multi-task transformer model, which is the main focus of the text.\n\nAccording to the context, the answer to the question is:\n\nThe baseline is the \"baseline transformer BIBREF8\".",
    "evidence": [
      "In contrast, we focus on relevant terms rather than similar terms.\n\n\nResults and Analysis\nTable TABREF14 shows the effectiveness of our model (multi-task transformer) over the baseline transformer BIBREF8 . Our model achieves significant performance gains in the test sets over the baseline for both ",
      "tion needs from sentences and use the output of that model to get relevant documents.\n\n\nAcknowledgments\nThis work was supported in part by the Center for Intelligent Information Retrieval and in part by the Air Force Research Laboratory (AFRL) and IARPA under contract #FA8650-17-C-9118 under subcont",
      "arning component shares two layers with the NMT model. The goal is to expose the retrieval corpus' vocabulary to the NMT model. We discuss layer sharing in the next section. We select the single top document retrieved against a sentence INLINEFORM0 because a sentence is a weak representation of info",
      " olypmic, winter, games}. Transformer comes up with terms like parliament, also, two and years that never appears in human translation. We found that these terms are very likely in Europarl and rare in CLEF. Our model also generates terms such as harmonising, free, olympics that not generated by tra",
      "). We consider two data sources for learning NMT and RAT jointly. The first one is a sentence-level parallel corpus, which we refer to as translation corpus, INLINEFORM0 . The second one is the retrieval corpus, which is a collection of INLINEFORM1 documents INLINEFORM2 in the same language as INLIN"
    ]
  },
  {
    "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
    "answer": "Based on the provided context, the invertibility condition is mentioned in the \"Learning & Inference\" section. According to the text, the invertibility condition is satisfied when the neural projector is constrained with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Additionally, the text states that the invertible transformation satisfies INLINEFORM1, and its Jacobian matrix is triangular with all ones on the main diagonal, making the Jacobian determinant always equal to one (i.e., volume-preserving) and naturally satisfying the invertibility condition.",
    "evidence": [
      "l likelihood computation.\n\n\nLearning & Inference\nIn this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible trans",
      " nonlinear form. This transformation satisfies INLINEFORM1 , and BIBREF16 show that its Jacobian matrix is triangular with all ones on the main diagonal. Thus the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied. To be suffic",
      " the tractability of the underlying syntax model.\n\n\nInvertible Volume-Preserving Neural Net\nFor the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrar",
      "ng and invertibility properties. Such a sequence of invertible transformations from the data space INLINEFORM6 to INLINEFORM7 is also called normalizing flow BIBREF20 .\n\n\nExperiments\nIn this section, we first describe our datasets and experimental setup. We then instantiate our approach with Markov ",
      "avior of the projection. Maximizing it can be thought of as preventing information loss. In the extreme case, the Jacobian determinant is equal to zero, which means the projection is non-invertible and thus information is being lost through the projection. Such \u201cinformation preserving\u201d regularizatio"
    ]
  },
  {
    "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The authors propose a crowdsourcing method for lexicon acquisition, which involves both annotation and evaluation by the crowd.\n* To evaluate the quality of the lexicon, the crowd evaluates existing annotations, and their evaluations are compared to those of expert linguists.\n\nAccording to the context, the authors compare lexicons by evaluating the crowd's performance on evaluation, specifically by comparing evaluations from linguistic experts and the crowd itself.",
    "evidence": [
      "swers are considered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.\n\n\nData\nDuring January 2017, we pe",
      "ent analysis lexicons have been created by experts, such as linguists and behavioural scientists, with strict rules. Lexicon evaluation was also performed by experts or gold standards. In our paper, we propose a crowdsourcing method for lexicon acquisition, which is scalable, cost-effective, and doe",
      " to maintain an end to end automated work-flow for a crowdsourced (annotation and evaluation wise) lexicon acquisition process. Therefore, to highlight crowd's performance on evaluation, we compare evaluations from linguistic experts and the crowd itself.\n\n\nRelated Work\nAccording to BIBREF12 , an em",
      "m and description in the task confused workers, and were excluded from the interface. We tested several interface designs, and the one that worked best had minimal instructions. Lexicon acquisition interfaces in paid micro-task environments should be further studied, with regards to various other co",
      "high agreement evaluation is applied, the crowd and expert evaluations are almost identical. The number of crowd evaluations is the factor that provides a degree of freedom in the evaluation strictness.\n\n\nLimitations\nLexicon acquisition is a complex task that includes a mixture of objective and subj"
    ]
  },
  {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* KAR is a model that is comparable in performance with state-of-the-art MRC (Machine Reading Comprehension) models.\n* KAR is superior to state-of-the-art MRC models in terms of robustness to noise and hunger for data.\n\nFrom the context, I can conclude that KAR is a type of MRC model.",
    "evidence": [
      " experimental results, KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. The reasons for these achievements, we believe, are as follows:\n\n\nConclusion\nIn this paper, we innovative",
      "at is to say, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. To verify the effectiveness of general knowledge, we first study the relationship between the amount of general knowledge and the performance of KAR. As shown in Tab",
      "hat KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. In the future, we plan to use some larger knowledge bases, such as ConceptNet and Freebase, to improve the quality and scope",
      "odels. As shown in Table TABREF12 , on the development set and the test set, the performance of KAR is on par with that of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state-of-the-art MRC models by a large margin. That is to say, KAR is comparable in performance wit",
      "erforms much better than SAN and QANet. That is to say, when only a subset of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.\n\n\nAnalysis\nAccording to the experimental results, KAR is not only comparable "
    ]
  },
  {
    "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The proposed algorithm encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension.\n* The authors modify the learning objective to minimize a weighted average distance for words under the same semantic concept.\n* The 32nd dimension values are encoded with the concept JUDGMENT, resulting in an increase in dimension values for words such as committee, academy, and article.\n\nAccording to the context, the semantically related words take larger values along the 32nd dimension, which is encoded with the concept JUDGMENT.\n\nAnswer: The semantically related words take larger values along the 32nd dimension.",
    "evidence": [
      "m that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related",
      "or the different relations extracted from a semantic lexicon such that the word pairs that satisfy the same relation are distributed more homogeneously. For the latter, the authors modify the learning objective such that some weighted average distance is minimized for words under the same semantic c",
      "proposed algorithm, the 32nd dimension values are encoded with the concept JUDGMENT, which is reflected as an increase in the dimension values for words such as committee, academy, and article. We note that these words (red) are not part of the pre-determined word-group for the concept JUDGMENT, in ",
      "e concept of JUDGMENT on its designated vector dimension above and beyond the supplied list of words belonging to the concept word-group for that dimension. We also present the list of words with the greatest dimension value for the dimensions 11, 13, 16, 31, 36, 39, 41, 43 and 79 in Table TABREF11 ",
      " corresponding to the words are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words t"
    ]
  },
  {
    "title": "Stance Detection in Turkish Tweets",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The study focuses on stance detection for sports-related events.\n* The targets are two popular football clubs in Turkey, specifically Galatasaray (Target-1) and Fenerbah\u00e7e (Target-2).\n\nAnswer: The targets are Galatasaray (Target-1) and Fenerbah\u00e7e (Target-2).",
    "evidence": [
      "eing published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance de",
      "the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered ",
      "This stance class and even finer-grained classes can be considered in further annotation studies. We should also note that in a few tweets, the target of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised. Still, we have consid",
      "on the current study, and finally Section SECREF5 concludes the paper with a summary.\n\n\nA Stance Detection Data Set\nWe have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every ",
      "ub is praised or criticised. Still, we have considered the club as the target of the stance in all of the cases and carried out our annotations accordingly. At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and s"
    ]
  },
  {
    "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The EEG data is collected from 14 participants.\n* Each participant is presented with the same prompts 11 times.\n\nAccording to the context, the EEG data comes from 14 subjects.",
    "evidence": [
      "f 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the ",
      " such complex feature representations from single-trial EEG data. Besides, we found that instead of using the raw multi-channel high-dimensional EEG requiring large training times and resource requirements, it is advantageous to first reduce its dimensionality by capturing the information transfer a",
      "rodes\nMultichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space. In fact, our investigation based on a development set (as we ",
      "cheme for a number of binary classification tasks from speech imagery EEG data. Unlike previous approaches which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological cat",
      "has demonstrated promising potential to differentiate between various brain activities through measurement of related electric fields. EEG is non-invasive, portable, low cost, and provides satisfactory temporal resolution. This makes EEG suitable to realize BCI systems. EEG data, however, is challen"
    ]
  },
  {
    "title": "Towards Understanding Neural Machine Translation with Word Importance",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors test their word importance approach on two representative model architectures.\n* The specific model architectures are not explicitly mentioned in the context.\n\nSince the context does not provide the specific model architectures, I cannot find the answer from the given context.",
    "evidence": [
      "n with black-box methods on a couple of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve NMT mod",
      " the notion of \u201cword importance\u201d is employed to quantify the contribution that a word in the input sentence makes to the NMT generations. We categorize the methods of word importance estimation into two types: black-box methods without the knowledge of the model and white-box methods that have acces",
      "For instance, a better architecture for a given language pair should consider its own language characteristics.\n\n\nIntroduction ::: Contributions\nOur main contributions are: Our study demonstrates the necessity and effectiveness of exploiting the intermediate gradients for estimating word importance.",
      "tput behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on iden",
      "rmediate gradients for estimating word importance. We find that word importance is useful for understanding NMT by identifying under-translated words. We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be consi"
    ]
  },
  {
    "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context mentions that static embeddings are created by taking the first principal component of a word's contextualized representations.\n* It does not provide further details on how to calculate the static embedding for each word.\n\nAnswer: They calculate a static embedding for each word by taking the first principal component of its contextualized representations.",
    "evidence": [
      "till, static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks. These insights help justify why the use of contextualized representations has led to such significant improvemen",
      " Though they learn embeddings iteratively in practice, it has been proven that in theory, they both implicitly factorize a word-context matrix containing a co-occurrence statistic BIBREF7, BIBREF8. Because they create a single representation for each word, a notable problem with static word embeddin",
      "yer of ELMo, BERT, or GPT-2 can more than 5% of the variance in a word's contextualized representations be explained by a static embedding, on average. Though not visible in Figure FIGREF29, the raw MEV of many words is actually below the anisotropy baseline: i.e., a greater proportion of the varian",
      "dings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table TABREF34, we plot the performance of these PC static embeddings on several benchmark tasks. These tasks cover semantic similarity, analogy solving, and concept categoriza",
      "rtion of variance in $w$'s contextualized representations for a given layer that can be explained by their first principal component. It gives us an upper bound on how well a static embedding could replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poor"
    ]
  },
  {
    "title": "Paraphrase-Supervised Models of Compositionality",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The text discusses the use of compositionality features in machine translation.\n* It mentions the development of an English-Spanish MT system using the cdec decoder.\n* The system is evaluated for its impact on translation quality, showing improvements.\n\nAs for the question, I found the following information:\n\nThe text does not explicitly mention which translation systems they compare against. However, it does mention that their approaches are evaluated for their impact on a machine translation system, and that they show improvements in translation quality. This suggests that they are comparing their system against previous approaches, but the specific systems are not mentioned.\n\nAnswer: I cannot find the answer from the given context.",
    "evidence": [
      "s with equivalent performance to previous approaches that have relied on hand-annotated training data. Furthermore, compositionality features consistently improve the translations produced by a strong English\u2013Spanish translation system.\n\n\nParametric Composition Functions\nWe formalize composition as ",
      "iple sources of information can be used to learn partial semantic supervision that matches previous techniques in intrinsic evaluation tasks. Our approaches are also evaluated for their impact on a machine translation system where we show improvements in translation quality, demonstrating that compo",
      "r translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and decoding). Corpora from the WMT 2011 evaluation was used to build the translation and langua",
      "ng under the hypothesis that sentences which express their meaning non-compositionally should also translate non-compositionally. Modern phrase-based translation systems are faced with a large number of possible segmentations of a source-language sentence during decoding, and all segmentations are c",
      "ositionality score (either because it consists of non-terminals only or the lexical items in the translation rule are unigrams), and correspondingly the second feature indicates if the translation rule has been scored. Therefore, an appropriate additional baseline would be to mark translation rules "
    ]
  },
  {
    "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The model performs explicit matrix factorization to learn subword and word representations.\n* The model uses a word-context co-occurrence matrix, which is constructed by sliding a window of fixed size centered over every target word in the subsampled training corpus.\n\nAccording to the context, the matrix factorization method used is not explicitly mentioned. However, it is mentioned that the model performs explicit matrix factorization, which implies that it uses a specific method for matrix factorization. However, the context does not provide further details on the specific method used.\n\nTherefore, I cannot find the answer from the given context.",
    "evidence": [
      ", BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, BIBREF26 and BIBREF27 retrofit mor",
      "matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations BIBREF15 , BIBREF16 , BIBREF17",
      "gies BIBREF2 and the Microsoft syntactic analogies (MSR) dataset BIBREF30 . We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and nat",
      "he word-context co-occurrence matrix constructed by sliding a window of fixed size centered over every target word  $w$ in the subsampled BIBREF2 training corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair). LexVec adjusts the PPMI m",
      " counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , but the method proposed here should transfer to GloVe unchanged. The LexVec objective is modified such that a word's vector is the sum of all its subword vec"
    ]
  },
  {
    "title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The paper explores improving the performance of neural abstractive summarization models when applied to small corpora.\n* The authors propose three approaches: domain transfer, data synthesis, and combining both directions.\n\nAs for the recent abstractive summarization method mentioned in the paper, it is not explicitly stated. However, the paper does mention \"recent abstractive summarization methods\" in the abstract, which suggests that the authors are building upon existing methods in the field. However, the specific method(s) used in the paper are not specified.\n\nTherefore, I cannot find the answer to this question from the given context.",
    "evidence": [
      "th directions. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.\n\n\nRelated Work\nAbstractive Summarization. Abstractive summarization aims to generate coherent summaries with high readability, and has seen increasing i",
      "Abstract\nTraining abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corp",
      "l. Second, we propose a template-based synthesis method to create synthesized summaries, then explore the effect of enriching training data for abstractive summarization using the proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural abstractive s",
      " can remedy the size of in-domain data and improve performance.\n\n\nConclusions and Future Work\nWe explored improving the performance of neural abstractive summarizers when applied to the low resource domain of student reflections using three approaches: domain transfer, data synthesis and the combina",
      "omain transfer for abstractive summarization. While domain transfer is not new, compared to prior summarization studies BIBREF6, BIBREF7, our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is small. Second, we propose a template-based synthesis m"
    ]
  },
  {
    "title": "Revisiting Summarization Evaluation for Scientific Articles",
    "answer": "Based on the provided context, the key points are:\n\n* ROUGE is not accurate in quantifying the quality of a candidate summary, especially in summaries with high terminology variations and paraphrasing.\n* ROUGE may not be the best metric for summarization evaluation.\n* Different variants of ROUGE result in different correlation values with human judgments.\n\nThe common belief that this paper refutes is that ROUGE is a reliable metric for evaluating the quality of a summary. The paper suggests that ROUGE is not much reliable, contrary to this common belief.",
    "evidence": [
      "a relatively long research paper, they might use different terminology and paraphrasing. Therefore, Rouge which only relies on term matching between a candidate and a gold summary, is not accurate in quantifying the quality of the candidate summary.\n\n\nCorrelation of Sera with Rouge\nTable TABREF25 sh",
      "owed that Rouge may not be the best metric for summarization evaluation; especially in summaries with high terminology variations and paraphrasing (e.g. scientific summaries). Furthermore, we showed that different variants of Rouge result in different correlation values with human judgments, indicat",
      "t correlation values with human judgments, indicating that not all Rouge scores are equally effective. Among all variants of Rouge, Rouge-2 and Rouge-3 are better correlated with manual judgments in the context of scientific summarization. We furthermore proposed an alternative and more effective ap",
      "n has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summ",
      "few of these variants are often chosen (arbitrarily) for evaluation of the quality of the summarization approaches. When Rouge was proposed, the original variants were only recall-oriented and hence the reported correlation results BIBREF1 . The later extension of Rouge family by precision were only"
    ]
  },
  {
    "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The paper reports improvements over baseline systems.\n* The baseline systems are not explicitly mentioned, but it is mentioned that the paper uses a baseline system that is only trained on parallel data.\n\nBased on the summarized information, the answer to the question is:\n\nThe baseline systems are not explicitly mentioned in the context, but it is mentioned that one of the baseline systems is only trained on parallel data.",
    "evidence": [
      "ta, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which",
      "lel and synthetic data, compared to the baseline that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3\u20131.7 Bleu). Our submitted system includes reranking of the 50-best output of the left-to-right model with a right-to-left model \u2013 again a",
      "ed the ensemble of the last 4 checkpoints on dev, and we thus submitted the best single system as primary system.\n\n\nEnglish\u2194\\leftrightarrow Russian\nFor English INLINEFORM0 Russian, we cannot effectively learn BPE on the joint vocabulary because alphabets differ. We thus follow the approach described",
      "anking systems is stronger than an ensemble created from two independent l-r runs.\n\n\nEnglish\u2194\\leftrightarrow German\nTable TABREF13 shows results for English INLINEFORM0 German. We observe improvements of 3.4\u20135.7 Bleu from training with a mix of parallel and synthetic data, compared to the baseline t",
      "as the smallest amount of parallel training data, and we found dropout to be very effective, yielding improvements of 4\u20135 Bleu. We found that the use of diacritics was inconsistent in the Romanian training (and development) data, so for Romanian INLINEFORM0 English we removed diacritics from the Rom"
    ]
  },
  {
    "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
    "answer": "Based on the provided context, here is the summary of key points:\n\n* The authors believe that there is still space for improvement in a system that answers questions.\n* They conducted a human study to examine the room for improvement in their own small study.\n* The human study showed that there is still space for improvement on the CBT beyond the performance of their model.\n\nAccording to the context, the authors show there is space for further improvement by conducting a human study that demonstrates the room for improvement on the CBT beyond the performance of their model.\n\nAnswer: They show there is space for further improvement by conducting a human study that demonstrates the room for improvement on the CBT beyond the performance of their model.",
    "evidence": [
      "We then show in our own human study that there is still space for further improvement.\n\n\nIntroduction\nSince humans amass more and more generally available data in the form of unstructured text it would be very useful to teach machines to read and comprehend such data and then use this understanding ",
      "es that can be answered by machines and not by humans; 2) there is still space for improvement. A system that would answer correctly every time when either our ensemble or human answered correctly would achieve accuracy over 92% percent on both validation and test NE datasets and over 96% on both CN",
      " Given that we believe that if the community is striving to bring the performance as far as possible, it should move its work to larger data. This thinking goes in line with recent developments in the area of language modelling. For a long time models were being compared on several \"standard\" datase",
      "d by Facebook. However we hypothesized that the system is still not achieving its full potential so we decided to examine the room for improvement in our own small human study.\n\n\nHuman Study\nAfter adding more data we have the performance on the CBT validation and test datasets soaring. However is th",
      "n baseline as reported by Facebook BIBREF2 . However in the final section we show in our own human study that there is still room for improvement on the CBT beyond the performance of our model.\n\n\nTask Description\nA natural way of testing a reader's comprehension of a text is to ask her a question th"
    ]
  },
  {
    "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
    "answer": "Based on the provided context, the keyword-specific expectation is elicited from the crowd through an iterative process. At each iteration, the crowd is asked to estimate the keyword-specific expectation, and then the model is trained using expectation regularization. The model's prediction is compared to the crowd's expectation, and the keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation are selected. This process is repeated to refine the keyword-specific expectation.",
    "evidence": [
      "h for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at",
      ", the keyword-specific expectation $e^{(t)}$, and the parameter of the target model $\\theta ^{(t)}$. The E-step and the crowd parameter update in the M-step are similar to the Dawid-Skene model BIBREF14. The keyword expectation is inferred by taking into account both the crowd-contributed labels and",
      "ng new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expect",
      "s. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is use",
      "tation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting p"
    ]
  },
  {
    "title": "ARAML: A Stable Adversarial Training Framework for Text Generation",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The discriminators of GAN baselines are implemented based on the original papers.\n* The generators of all baselines except LeakGAN are the same as the model being compared.\n* The generator of LeakGAN consists of a hierarchical LSTM unit, following the implementation in the original paper.\n\nThe answer to the question is:\n\nThe GAN models used as baselines to compare against are MLE, SeqGAN, LeakGAN, IRL, MaliGAN, and ARAML.",
    "evidence": [
      " of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the original codes, because the convergence of baselines is sensitive to these hy",
      " stable than other GAN baselines in both metrics. Although LeakGAN reaches the best forward perplexity, its standard deviation is extremely large and it performs badly in reverse perplexity, indicating that it generates limited expressions that are grammatical yet divergent from the data distributio",
      "As for the details of the baselines, the generators of all the baselines except LeakGAN are the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN bas",
      "f ARAML-R is still smaller than other GAN baselines.\n\n\nCase Study\nTable TABREF47 presents the examples generated by the models on COCO. We can find that other baselines suffer from grammatical errors (e.g. \u201cin front of flying her kite\" from MLE), repetitive expressions (e.g. \u201cA group of people\" from",
      "es and chose the best 5 trials for SeqGAN, LeakGAN, IRL, MaliGAN and ARAML, respectively. Then, we presented the forward/reverse perplexity in the training process in Figure FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics. "
    ]
  },
  {
    "title": "Predicting the Industry of Users on Social Media",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The researchers examined the task of predicting a social media user's industry.\n* They introduced an annotated dataset of over 20,000 blog users.\n* The study used a content-based classifier in conjunction with two feature selection methods.\n* The dataset was verified by the Sina Weibo platform.\n\nAccording to the context, the social media platform that was looked at is Sina Weibo.",
    "evidence": [
      " At the same time, a number of researchers sought to predict the social media users' age and/or gender BIBREF14 , BIBREF15 , BIBREF16 , while others targeted and analyzed the ethnicity, nationality, and race of the users BIBREF17 , BIBREF18 , BIBREF19 . One of the profile fields that has drawn a gre",
      "Abstract\nAutomatic profiling of social media users is an important task for supporting a multitude of downstream applications. While a number of studies have used social media content to extract and study collective social attributes, there is a lack of substantial research that addresses the detect",
      "aper, we examined the task of predicting a social media user's industry. We introduced an annotated dataset of over 20,000 blog users and applied a content-based classifier in conjunction with two feature selection methods for an overall accuracy of up to 0.534, which represents a large improvement ",
      "ak this information, a number of studies have utilized social media users' footprints to approximate their profiles. This paper explores the potential of predicting a user's industry \u2013the aggregate of enterprises in a particular field\u2013 by identifying industry indicative text in social media. The acc",
      ". This work examined only users that have been verified by the Sina Weibo platform, introducing a potential bias in the resulting dataset. Finally, Preotiuc-Pietro et al. Preoctiuc15 predicted the occupational class of Twitter users using the Standard Occupational Classification (SOC) system, which "
    ]
  },
  {
    "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
    "answer": "Based on the provided context, the model overcomes the assumption that all words in a document are generated from a single event by proposing an event extraction model (AEM) that employs a generator network to learn the projection function between an event distribution and event-related word distributions (entity, location, keyword, date). This approach captures event-related patterns rather than generating text sequences, allowing the model to handle long texts like news articles that describe multiple events.",
    "evidence": [
      "Abstract\nTo extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assump",
      "ations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) D",
      "e become important sources of real-world events. Therefore, it is crucial to automatically extract events from online texts. Due to the high variety of events discussed online and the difficulty in obtaining annotated data for training, traditional template-based or supervised learning approaches fo",
      "true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extract",
      "a generator network is employed in AEM to learn the projection function between an event distribution and the event-related word distributions (entity, location, keyword, date). The learned generator captures event-related patterns rather than generating text sequence; (2) Different from LEM and DPE"
    ]
  },
  {
    "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* MSD prediction is a task that involves predicting the Morpho-Syntactic Description (MSD) tag of a target form.\n* The task is conditioned on the context encoding, as described in UID15.\n* MSD tag predictions are generated with an LSTM one component at a time.\n\nBased on this summary, the answer to the question is:\n\nMSD prediction is a task that involves predicting the Morpho-Syntactic Description (MSD) tag of a target form, conditioned on the context encoding.",
    "evidence": [
      " increase the morpho-syntactic awareness of the encoder and to regularise the learning process\u2014the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are generated with an LSTM one component at a time, e.g. the",
      "le data augmentation techniques that encourage autoencoding BIBREF2 .\n\n\nMSD prediction\nFigure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above. Accuracy here is generally higher than on the main task, with the multilingual finetuned setup for ",
      "'s in Figure FIGREF23 . One notable exception to this pattern is fr where inflection gains a lot from multilingual training, while MSD prediction suffers greatly. Notice that the magnitude of change is not always the same, however, even when the general direction matches: for ru, for example, multil",
      "ased encoder-decoder model: (1) a wide context window, encoding the entire available context; (2) multi-task learning with the auxiliary task of MSD prediction, which acts as a regulariser; (3) a multilingual approach, exploiting information across languages. In future work we aim to gain better und",
      "nly. The results indicate that encoding the full context with an LSTM highly enhances the performance of the model, by 11.15% on average. This observation explains the high results we obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for four languages "
    ]
  },
  {
    "title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The model uses a large biomedical data collection that includes knowledge bases like PubMed and PMC and a Wikipedia dump of biomedical-related articles.\n* The model does not use supervised methods due to insufficient training data.\n* The model uses general embeddings trained from the biomedical data collection.\n\nAccording to the context, the answer to the question is:\n\nThe dataset used to build the model is a large biomedical data collection that includes knowledge bases like PubMed and PMC and a Wikipedia dump of biomedical-related articles.",
    "evidence": [
      "we choose the most semantically related candidate as the answer. General embeddings: Different from the Raw Input embeddings baseline, we use the embedding trained from a large biomedical data collection that includes knowledge bases like PubMed and PMC and a Wikipedia dump of biomedical related art",
      "pervised methods as in previous works mentioned since we do not have sufficient training data. And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. R",
      "ollowing baselines to compare with our approach. Rating: This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base. Raw Input embeddings: We trained word embeddings only from the 1,160 raw ICU texts and we choose the most semantically related candidate ",
      "ormation largely benefits the abbreviation expansion model. Compared with our approach, embeddings trained only from the ICU texts do not significantly contribute to the performance over the rating baseline. The reason is that the size of data for training the embeddings is so small that many candid",
      "kind of errors accounted for about 54%. One possible solution may be adding more effective data to the embedding training, which means discovering more task-oriented resources. In a few cases, we failed to identify some abbreviations because of their complicated representations. For example, we have"
    ]
  },
  {
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "answer": "Based on the provided context, the nine types of user reactions mentioned are:\n\n1. Answer\n2. Appreciation\n3. Elaboration\n4. Question\n5. Disagreement\n6. Humor\n7. Negative reaction\n8. Other (no label predicted)\n\nNote that the context does not explicitly mention the ninth type, but it can be inferred from the sentence \"we develop a model to classify user reactions into one of nine types...\"",
    "evidence": [
      "actions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call \u201cother\u201d, using linguistically-infused neural network models.\n\n\nReddit Data\nWe use a manually annotated Reddit",
      "asets, we found that the primary reaction types were answer, appreciation, elaboration, question, or \u201cother\u201d (no label was predicted). Figure FIGREF13 illustrates the distribution of reaction types among Reddit comments (top plot) or tweets (bottom plot) responding to each type of source, as a perce",
      "ews sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sour",
      "nd then assigned a label to each tweet based on the class of the source @mentioned or retweeted. A breakdown of each dataset by source type is shown in Table TABREF10 . Figure FIGREF11 illustrates the distribution of deceptive news sources and reactions across the four sub-categories of deceptive ne",
      "ource type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility.\n\n\nResults and Discussion\nFor both Twitter and Reddit datasets, we found that the primary reaction types we"
    ]
  },
  {
    "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The paper presents a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy.\n* The architecture has lower layers supervised by downstream tasks and the final layer task is an auxiliary unsupervised task.\n\nAccording to the context, there is no specific mention of the exact number of supervised tasks used. However, it is mentioned that the architecture has \"lower layers\" which are supervised by downstream tasks, implying that there is more than one supervised task. Therefore, the answer to the question is:\n\nI cannot find the answer from the given context.",
    "evidence": [
      "f supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two perc",
      "aining on unlabelled data (but there is the LM) and confirm previous results that arranging tasks in a hierarchy improves performance. Results for both models can be seen for POS in Table 2 and for Chunk in Table 1 .\n\n\nSemi-Supervised Experiments\nExperiments showing the effects of our semi-supervise",
      "pervised auxiliary task (e.g. plank2016multilingual) as the most senior layer we can use this structure for semi-supervised learning - the error on the unsupervised tasks penalises junior tasks when their representations and predictions are not consistent. It is the aim of this paper to demonstrate ",
      "Abstract\nIn this paper we present a novel Neural Network algorithm for conducting semisupervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of",
      "ations of label tags. In addition to sogaard2016deep. Work such as mirowski-vlachos:2015:ACL-IJCNLP in which incorporating syntactic dependencies improves performance, demonstrates the benefits of incorporating junior tasks in prediction. Our neural network has one hidden layer, after which each suc"
    ]
  },
  {
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The authors explore Twitter data related to health and extract popular topics under different categories (e.g., diet, exercise) using topic modeling.\n* They observe interesting hidden correlations in the data, including the correlation between \"Yoga-Veganism\" and \"Women-Yoga\".\n* The authors also mention that they would say there are noticeable correlations in the data, but they do not specify what these correlations are.\n\nAs for the question, the context mentions that the authors observe \"interesting hidden correlation in data\" and specifically mention the correlation between \"Yoga-Veganism\" and \"Women-Yoga\". However, it does not provide information on what other interesting correlations are observed.\n\nTherefore, I can only answer based on the provided context:\n\nThe context does not mention any other interesting correlations observed, apart from the correlation between \"Yoga-Veganism\" and \"Women-Yoga\".",
    "evidence": [
      "r over one of the bubbles (Fig. FIGREF21 ), the words and bars on the right-hand side have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected to",
      "nknown correlations, find out the dominant topic in data and understand people's interest through the discussions. In this work, we explore Twitter data related to health. We extract the popular topics under different categories (e.g. diet, exercise) discussed in Twitter via topic modeling, observe ",
      "a more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'.\n\n\nTopic Frequency Distribution\nEach tweet is composed of multiple topics. But, typically only one of the topics is dominant. ",
      "Abstract\nNowadays social media is a huge platform of data. People usually share their interest, thoughts via discussions, tweets, status. It is not possible to go through all the data manually. We need to mine the data to explore hidden patterns or unknown correlations, find out the dominant topic i",
      "ta and it's not possible to go through all the data manually. We need to mine the data to get overall statistics and then we will also be able to find some interesting correlation of data. Several works have been done on prediction of social media content BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBR"
    ]
  },
  {
    "title": "Learning Relational Dependency Networks for Relation Extraction",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The text discusses the benefits of joint learning in relation to learning relations between entities.\n* It is mentioned that some relations, such as parents, spouse, siblings, etc., are inter-related and learning them jointly indeed improves performance.\n\nAccording to the context, the answer to the question is:\n\nThey learn features, with gold standard examples only (i.e., no weak supervision, word2vec, advice, or semi-supervised learning).",
    "evidence": [
      "arning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them",
      "features, with gold standard examples only (i.e., no weak supervision, word2vec, advice, or advice). Since our system had different components, we aimed to answer the following questions:\n\n\nJoint learning\nTo address our next question, we assessed our pipeline when learning relations independently (i",
      "g useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines. However, some surprising observations include the fact that weak supervision is not as useful as expected and word2vec features are not as predic",
      " siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations.\n\n\nword2vec\nTable TABREF24 shows the results of experiments comparing the RDN framework with and without word2vec features. word2vec appears to largel",
      "ertain patterns and use them to deduce the nature of the relation between two entities present in the text. The goal of our work is to capture such mental patterns of the humans as advice to the learning algorithm. We modified the work of Odom et al. odomAIME15,odomAAAI15 to learn RDNs in the presen"
    ]
  },
  {
    "title": "Simultaneous Neural Machine Translation using Connectionist Temporal Classification",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors conducted experiments in English-to-Japanese simultaneous translation.\n* They used two metrics to evaluate translation accuracy: BLEU and RIBES.\n* They used token-level delay to measure latency.\n\nBased on these key points, the answer to the question is:\n\nThe authors use BLEU, RIBES, and token-level delay to evaluate simultaneous translation.",
    "evidence": [
      "Abstract\nSimultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency. We have to determine when we start the translation for observed inputs so far, to achiev",
      "ognition, where we have a reference word sequence but do not have the corresponding segmentation or alignment in an acoustic signal. We conduct experiments in English-to-Japanese simultaneous translation with the proposed and baseline methods and show the proposed method achieves a good translation ",
      "used along with the latency penalty and a standard word prediction loss. The experimental results suggest the proposed method determines when to translate or when to wait in an adaptive manner. Future work includes further analyses on translation accuracy in different latency conditions and time-bas",
      "e used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese translation accuracy.\n\n\nExperiments ::: Experiments with Small-scale Corpus\nWe conducted small-scale experiments us",
      "w the proposed method achieves a good translation performance with relatively small latency. The proposed method can determine when to wait or translate in an adaptive manner and is useful in simultaneous translation tasks.\n\n\nSimultaneous machine translation by Wait-k model\nFirst, we review a genera"
    ]
  },
  {
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The system uses a classifier model for emergency detection.\n* The classifier model is used as a filter for the second stage of classification.\n* The classification training for Stage 1 was performed using two traditional classification techniques: SVM (Support Vector Machine) and NB (Naive Bayes).\n\nAccording to the context, the classifier used for emergency detection is SVM (Support Vector Machine).",
    "evidence": [
      " We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization",
      "urrent system performs compression, normalization and spell-checking if the language used is English. The classifier training and detection process are described below.\n\n\nEmergency Classification\nThe first classifier model acts as a filter for the second stage of classification. We use both SVM and ",
      ". We also provide the system with sample tweets in real time and assess its ability to detect the emergency, and classify it accordingly. The classification training for Stage 1 was performed using two traditional classification techniques SVM and NB. SVM outperformed NB by around INLINEFORM1 and be",
      "dress the above challenges and present our approach in the next section.\n\n\nOur Approach\nWe propose a software architecture for Emergency detection and visualization as shown in figure FIGREF9 . We collect data using Twitter API, and perform language pre-processing before applying a classification mo",
      "Abstract\nWe present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like\"fire\",\"accident\",\"earthquake\", etc. We demonstrate our ideas by classifying Twitter p"
    ]
  },
  {
    "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The authors, who are researchers in either NLP or computer security, did all of the annotation.\n* The annotation process involved multiple annotators, and the data was annotated at the token level to make the annotation as general as possible.\n* The authors preprocessed the data using a tokenizer and sentence-splitter.\n\nAnswer: The authors, who are researchers in either NLP or computer security, annotated the data.",
    "evidence": [
      "bers (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from ",
      "ry post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The data annotated during this process is not included in Tab",
      "ifferent cybercrime forums seem to represent different \u201cfine-grained domains.\u201d Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation BIBREF12 , BIBREF13 , token-level annotation BIBRE",
      ". Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for",
      "nnotated a subset of them. In total, we annotated 130,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data. Figure FIGREF2 shows two examples of posts from Darkode. In addition to aspects of the annotation, which we describe bel"
    ]
  },
  {
    "title": "Deep Health Care Text Classification",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* Recurrent neural network (RNN) is introduced as an enhanced model of feed forward network (FFN) in 1990.\n* RNN is used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories.\n* The paper proposes RNN and LSTM based embedding method.\n\nAccording to the context, the type of RNN used is not specified as a specific variant, but rather referred to as \"Recurrent neural network (RNN)\".\n\nAnswer: The type of RNN used is not specified as a specific variant, but rather referred to as \"Recurrent neural network (RNN)\".",
    "evidence": [
      "t\u2019s variant\nRecurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 . The input sequences ${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden stat",
      "RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social Media Mining for Health Applications Shared Task at AMIA 2017. The experiment results are cons",
      "e presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and ",
      "ets of task 1 and task 2 are limited, this paper proposes RNN and LSTM based embedding method.\n\n\nBackground and hyper parameter selection\nThis section discusses the concepts of tweet representation and deep learning algorithms particularly recurrent neural network (RNN) and long short-term memory (L",
      ".1 which removes the neurons and its connections randomly. This acts as a regularization parameter. In task 1 the output layer contains $sigmoid$ activation function and $softmax$ activation function for task 2.\n\n\nRecurrent neural network (RNN) and it\u2019s variant\nRecurrent neural network (RNN) was an "
    ]
  },
  {
    "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    "answer": "Based on the provided context, here is a summary of the key points:\n\n* The text discusses the performance of a model for evidence-based claim veracity prediction.\n* The model achieves a perfect Micro and Macro F1 score of 1.0 on certain domains (ranz, bove, buca, fani, and thal).\n* The model performs well when the claims are short and specific topics tend to co-occur with correct predictions.\n\nAs for the question, the context does not explicitly mention the baselines used in the study. However, it does mention that the model achieves a perfect F1 score on certain domains, which implies that the model is performing well compared to some unknown baseline. Therefore, I will answer the question as follows:\n\nI cannot find the answer from the given context.",
    "evidence": [
      " for label, categories, person making the claim (speaker), person fact checking the claim (checker), tags, article title, publication date, claim date, as well as the full text that appears when the claim is clicked. Lastly, the above full text contains hyperlinks, so we further crawled the full tex",
      "h small numbers of instances, seem to be very easy \u2013 a perfect Micro and Macro F1 score of 1.0 is achieved on ranz, bove, buca, fani and thal. We find that for those domains, the verdict is often already revealed as part of the claim using explicit wording. Our evidence-based claim veracity predicti",
      "ns, whereas more specific tags such as `brisbane-4000' or `hong-kong' tend to co-occur with correct predictions. Similar trends are observed for bigrams. This means that the model has an easy time succeeding for instances where the claims are short, where specific topics tend to co-occur with certai",
      "ch snippets also have the benefit that they already contain summaries of the part of the page content that is most related to the claim. Our problem is to obtain encodings for INLINEFORM0 examples INLINEFORM1 . For simplicity, we will henceforth drop the task superscript and refer to instances as IN",
      "del is not entirely straight-forward. One option would be to manually map those labels onto one another. However, since the sheer number of labels is rather large (165), and it is not always clear from the guidelines on fact checking websites how they can be mapped onto one another, we opt to learn "
    ]
  },
  {
    "title": "Retrieval-based Goal-Oriented Dialogue Generation",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors compared their retrieval-based model to various semantically conditioned models explicitly using past dialog act information.\n* The authors did not provide specific names of the semantically conditioned models they compared with.\n\nAccording to the context, the answer to the question is:\n\n\"I cannot find the answer from the given context.\"",
    "evidence": [
      "xperiments conditioning both on predicted acts as well as ground truth past acts. We include both of these as well as the performance of our baseline and proposed model in table TABREF15. We divide the results into models using act information to condition the language generation and models that do ",
      ", neural encoder-decoder architecture leads to significant improvements, including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find",
      "cative of model performance than embedding similarity. We present some example outputs in table TABREF16. As mentioned earlier, from manual inspection of the outputs we observed that the the exemplar model is able to stay within the correct domain of the conversation and returns information within t",
      "dition the language generation and models that do not.\n\n\nDiscussion\nAs shown in table TABREF7, our simplest proposed model achieved the largest improvements over the baseline when it came to the average embedding similarity and vector extrema similarity. As it is hard to interpret what the differenc",
      "he most informative prior turns and conditions on those. This simple approach increases inductive bias and alleviates problems arising from long-context dependencies. We show that an information retrieval step leads to improvements over traditional dialogue generation models intended for open ended "
    ]
  },
  {
    "title": "User Generated Data: Achilles' heel of BERT",
    "answer": "Based on the provided context, the reason behind the drop in performance using BERT is the introduction of noise (spelling mistakes) in the data. The context states that when there is no error (0.0% error), the performance of BERT is very good for each of the three tasks. However, as the amount of noise in the data increases, the performance of BERT drops drastically. The context also mentions that the problem lies with how misspelt words are tokenized to create a representation of the text, which degrades further, resulting in the overall performance drop.",
    "evidence": [
      "ident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT proces",
      "o a significant drop in performance of the BERT model for the task at hand as compared to clean dataset. We further show that as we increase the amount of noise in the data, the performance degrades sharply.\n\n\nRelated Work\nIn recent years pre-trained language models ((e.g. ELMoBIBREF5, BERTBIBREF0) ",
      "e performance of BERT. We demonstrated that as the noise increases, BERT\u2019s performance drops drastically. We further investigated the BERT system to understand the reason for this drop in performance. We show that the problem lies with how misspelt words are tokenized to create a representation of t",
      "f the tokenizer degrades further, resulting in the overall performance drop. Our results and analysis shows that one cannot apply BERT blindly to solve NLP problems especially in industrial settings. If the application you are developing gets data from channels that are known to introduce noise in t",
      " negatively impacts the performance of BERT when working with user generated text data and evaluate the same. This evaluation is motivated from the business use case we are solving where we are building a dialogue system to screen candidates for blue collar jobs. Our candidate user base, coming from"
    ]
  },
  {
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "answer": "Summary of key points:\n\n* The analysis studies the cohesion within and between political groups in the Eighth European Parliament (2014-2019).\n* The cohesion is analyzed through two aspects: co-voting patterns and social behavior.\n* The study explores the level of cohesion of the European Parliament's groups and examines two important policy areas.\n\nAnswer:\n\nThe analysis provides insights into the cohesion of political groups in the European Parliament, both overall and across different policy areas.",
    "evidence": [
      "Abstract\nWe study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes. On one hand, we analyze their ",
      "aper we study the cohesion and coalitions exhibited by political groups in the Eighth European Parliament (2014\u20132019). We analyze two entirely different aspects of how the Members of the European Parliament (MEPs) behave in policy-making processes. On one hand, we analyze their co-voting patterns an",
      "nsights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. (ii) We explore whether coalitions are formed in the same way for different policy areas. (iii) We explore to what degree the retweeting behavior of MEPs corresponds to th",
      ". As already explained, we attribute this to the degree of Euroscepticism that these groups share on this particular policy issue.\n\n\nConclusions\nIn this paper we analyze (co-)voting patterns and social behavior of members of the European Parliament, as well as the interaction between these two syste",
      "f political groups\nIn this section we first report on the level of cohesion of the European Parliament's groups by analyzing the co-voting through the agreement and ERGM measures. Next, we explore two important policy areas, namely Economic and monetary system and State and evolution of the Union. F"
    ]
  },
  {
    "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The MS-MARCO dataset is different from the SQuAD dataset in several aspects.\n* The SQuAD dataset aims to answer a question with exact text spans in a passage, whereas the MS-MARCO dataset defines the task as answering a question from multiple passages.\n* In the SQuAD dataset, the answer is constrained to be an exact sub-span in the passage, whereas in the MS-MARCO dataset, the answer is not necessarily a sub-span of the passages.\n* The SQuAD dataset has only one passage for a question, whereas the MS-MARCO dataset has multiple passages.\n\nBased on these key points, I answer the question:\n\nMS-MARCO is different from SQuAD in that it allows answers that are not necessarily sub-spans of the passages, and it has multiple passages for a question, whereas SQuAD has only one passage and requires exact sub-spans as answers.",
    "evidence": [
      " only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer ",
      "he leaderboard of MS-MARCO. Experimental results show that our model achieves better results in official evaluation metrics. We also conduct ablation tests to verify our method, and compare our framework with the end-to-end generation framework.\n\n\nDataset and Evaluation Metrics\nFor the MS-MARCO data",
      "Abstract\nIn this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the word",
      " in existing machine reading comprehension datasets. First, the SQuAD dataset constrains the answer to be an exact sub-span in the passage, while words in the answer are not necessary in the passages in the MS-MARCO dataset. Second, the SQuAD dataset only has one passage for a question, while the MS",
      " not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on the MS-MARCO dataset follow their methods on the SQuAD. BIBREF7 combine match-LSTM and pointer networks to produce the boundary of the answer. BIBREF8 and BIBREF9 employ variant co-attention mechanism to "
    ]
  },
  {
    "title": "Neural Machine Translation with Supervised Attention",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The test set used is dataset as in BIBREF5.\n* The training data used is from the NIST2008 Open Machine Translation Campaign.\n* The development set used is nist02, and the test sets used are nist05, nist06, and nist07.\n\nAccording to the context, the dataset used is the NIST2008 Open Machine Translation Campaign, specifically the training data, development set, and test sets mentioned above.",
    "evidence": [
      " dataset as in BIBREF5 as the test set. Following BIBREF17 , we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment",
      "T2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in BIBREF0 for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units w",
      " and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively. We trained a 4-gram language model on the target side of training corpus for running Moses. For training all NMT systems, we employed the same settings as those in the large scale tas",
      " this way, we treat the attention variable INLINEFORM5 as an observable variable as shown in Figure 1(b), and this is different from the standard NMT as shown in Figure 1(a) in essence. Note that this training objective resembles to that in multi-task learning BIBREF11 . Our supervised attention met",
      "lementation.\n\n\nThe Large Scale Translation Task\nWe used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and ni"
    ]
  },
  {
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "answer": "Based on the provided context, I summarize the key points related to the question as follows:\n\n* The paper presents Watasense, an unsupervised system for word sense disambiguation.\n* The system uses a word sense inventory and supports both sparse and dense sense representations.\n* The dense approach is shown to substantially boost the performance of the system.\n* To evaluate the system, the authors use the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure.\n\nRegarding the question, the answer is:\n\nThe cosine similarity is used to estimate the sense of the word in a sentence.",
    "evidence": [
      "Abstract\nIn this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target w",
      "tances of this lemma.\n\n\nDataset\nWe evaluate the word sense disambiguation methods in Watasense against three baselines: an unsupervised approach for learning multi-prototype word embeddings called AdaGram BIBREF15 , same sense for all the instances per lemma (One), and one sense per instance (Single",
      "\n\nConclusion\nIn this paper, we presented Watasense, an open source unsupervised word sense disambiguation system that is parameterized only by a word sense inventory. It supports both sparse and dense sense representations. We were able to show that the dense approach substantially boosts the perfor",
      " by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure. In order to provide the overall value",
      " word embeddings. In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DIS"
    ]
  },
  {
    "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
    "answer": "Based on the provided context, the three datasets used are:\n\n1. M2M dataset\n2. DSTC2 dataset\n3. (The third dataset is not explicitly mentioned by name, but it is referred to as \"the training corpus\" in the context.)\n\nNote that the third dataset is not explicitly named, but it is mentioned as one of the datasets used for training and evaluating the models.",
    "evidence": [
      " we decided to use M2M dataset in our experiments to see how well models can handle a more diversed dataset.\n\n\nExperiments ::: Datasets ::: Dataset Preparation\nWe followed the data preparation process used for feeding the conversation history into the encoder-decoder as in BIBREF5. Consider a sample",
      "d via dialogue self-play and later augmented via crowdsourcing. We trained on our models on different datasets in order to make sure the results are not corpus-biased. Table TABREF12 shows the statistics of these three datasets which we will use to train and evaluate the models. The M2M dataset has ",
      "ataset and in our investigation of the trained model we found that this considerable reduction is due to the fact that the diversity of M2M dataset is considerably more compared to DSTC2 dataset while the traning corpus size is smaller.\n\n\nResults and Discussion ::: Time Performance Comparison\nTable ",
      "rain and evaluate the models. The M2M dataset has more diversity in both language and dialogue flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating task-oriented chatbots. This is also the reason that we decided to use M2M dataset in our experiments ",
      "nce modelling tasks and compare and analyze the results using different evaluation metrics on three different datasets. We provide insight into how effective are self-attentional models for this task and benchmark the time performance of these models against the recurrence-based sequence modelling m"
    ]
  },
  {
    "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for the customer service domain.\n* They design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations.\n\nAccording to the context, the dialogue acts that are more suited to the Twitter domain are the fine-grained dialogue acts developed for the customer service domain, which are tailored for Twitter conversations.",
    "evidence": [
      "earch, we require a set of dialogue acts that is more closely representative of customer service domain interactions - thus we expand upon previously defined taxonomies and develop a more fine-grained set. Modeling general conversation on Twitter has also been a topic of interest in previous work. H",
      "come-oriented conversational systems. We first expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for customer service, and conducting a systematic user study to identify these acts in a dataset of 800 conversations from four Twitter custome",
      ". We design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically overlapping, and conduct multi-label supervised learning experiments to predict multiple appro",
      "characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts. Core and Allen present the Dialogue Act Marking in Several Layers (DAMSL), a standard for discourse annotation that was ",
      "tems draw principles from many disciplines, including philosophy, linguistics, computer science, and sociology. In this section, we describe relevant previous work on speech and dialogue act modeling, general conversation modeling on Twitter, and speech and dialogue act modeling of customer service "
    ]
  },
  {
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The lexicon of trafficking flags can be expanded through word embeddings and t-SNE.\n* This approach also works for acronyms and deliberate typos.\n* The method uses word embeddings learned from raw text data to help expand the lexicon of trafficking flags.\n\nAnswer: The lexicon of trafficking flags is expanded through word embeddings and t-SNE, which uses word embeddings learned from raw text data to help expand the lexicon of trafficking flags.",
    "evidence": [
      "t on Trafficking-10K, an expert-annotated dataset of escort ads. Additionally, because traffickers use acronyms, deliberate typographical errors, and emojis to replace explicit keywords, we demonstrate how to expand the lexicon of trafficking flags through word embeddings and t-SNE.\n\n\nIntroduction\nG",
      "and we show that the lexicon of trafficking-related emojis can be subsequently expanded. In Section SECREF2 , we discuss related work on human trafficking detection and ordinal regression. In Section SECREF3 , we present our proposed model and detail its components. In Section SECREF4 , we present t",
      "in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.\n\n\nDiscussion\nHuman trafficking is a form of modern day slavery that victimizes millions of people. It has become the norm for sex traffickers to use escort websites to openly advertise their vi",
      "so conducted an emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of trafficking flags. Since our experiments, there have been considerable advancements in language representation models, such as BERT BIBREF30 . The new language representation",
      "on for ordinal regression neural networks, qualitatively analyzing patterns in the predicted samples, and expanding the trafficking lexicon through a data-driven approach are not dependent on a particular choice of language representation model. As for future work in trafficking detection, we can de"
    ]
  },
  {
    "title": "MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The model is evaluated on several benchmark datasets, including FB15K and FB15k-237.\n* FB15k-237 and WN18RR are used to test the expressiveness ability rather than relational pattern learning power of models.\n\nAccording to the context, the datasets used to evaluate the model are:\n\n* FB15K\n* FB15k-237\n* WN18\n* WN18RR\n\nAnswer: The datasets used to evaluate the model are FB15K, FB15k-237, WN18, and WN18RR.",
    "evidence": [
      " results of the models are close and the reported works make an unfair comparison of the dataset construction rather than the relational learning models. This is considerable when comparing results to ConvE that generates all possible combinations of object entities to generate samples(e.g., we obse",
      " empirical results show that MDE significantly improves the translating embeddings and outperforms several state-of-the-art embedding models on benchmark datasets.\n\n\nIntroduction\nWhile machine learning methods conventionally model functions given sample inputs and outputs, a subset of statistical re",
      " these models perform better than the TransE on FB15K(Table1), they produce lower rankings on FB15k-237(Table2) in the more fair comparison conditions.\n\n\nConclusion\nIn this study, we showed that not only some of the claimed limitations on the expressiveness of score based embeddings do not hold, but",
      " 1000, 1500, 2500, 3600. We set the initial learning rate on all datasets to 10. The best embedding size and $\\gamma _1$ and $\\gamma _2$ and $\\beta _1$ and $\\beta _2$ values on WN18 were 50 and 1.9, 1.9, 2 and 1 respectively and for FB15k were 100, 14, 14, 1, 1. The best found embedding size and $\\g",
      "o test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 BIBREF34 and WN18RR BIBREF35 exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. Baselines: We compare MDE with s"
    ]
  },
  {
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "answer": "Summary of key points:\n\n* The paper introduces innovative system designs and validation analysis.\n* Users' backstory queries about Gunrock are positively correlated to user satisfaction.\n* The relationship between word count and user engagement (overall rating, mean number of turns) was modeled in separate linear regressions.\n\nAnswer:\n\nAccording to the context, users' backstory queries about Gunrock are positively correlated to user satisfaction. This means that when users ask more backstory questions about Gunrock, their satisfaction with the conversation tends to be higher.",
    "evidence": [
      "in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries abou",
      "urns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.\n\n\nIntroduction\nAmazon Alexa Prize BIBREF0 provides a platform to collec",
      "odeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based o",
      "s.\n\n\nAnalysis ::: Gunrock's Backstory and Persona\nWe assessed the user's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., \u201cWhat's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regressi",
      " modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions. Results showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79"
    ]
  },
  {
    "title": "Image Captioning: Transforming Objects into Words",
    "answer": "Based on the provided context, the common captioning metrics mentioned are:\n\n* BLEU\n* ROUGE\n* SPICE\n\nThese metrics are commonly used to evaluate the performance of image captioning systems, and the context notes that BLEU and ROUGE have lower correlation with human judgments compared to the other metrics.",
    "evidence": [
      "uantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.\n\n\nIntroduction\nImage captioning\u2014the task of providing a natural language description of the content within",
      "le it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics.\n\n\nComparative Analysis\nWe compare our proposed algorithm aga",
      "s indeed benefit from incorporating spatial relationship information, most evidently when comparing the relevant sub-metrics of the SPICE captioning metric. We also present qualitative examples of how incorporating this information can yield captioning results demonstrating better spatial awareness.",
      "captioning. The authors explored extracting a single global image feature from the image as well as uniformly sampling features by dividing the image into 8x8 partitions. In the latter case, the feature vectors were fed in a sequence to the Transformer encoder. In this paper we propose to improve up",
      "e captioning has naturally followed suit. On the computer vision side, improved convolutional neural network and object detection architectures have contributed to improved image captioning systems. On the natural language processing side, more sophisticated sequential models, such as attention-base"
    ]
  },
  {
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The dataset was annotated using a crowdsourcing annotation framework.\n* The framework was used to annotate a large corpus of 12,000 QA pairs and natural language derivations.\n* The annotation process involved asking 3 crowdworkers to evaluate whether Natural Language Derivations (NLDs) can lead to a statement at four scale levels.\n\nBased on this information, the dataset was annotated through a crowdsourcing process, where multiple workers were asked to evaluate the relationship between NLDs and statements.\n\nAnswer: The dataset was annotated through a crowdsourcing process, where multiple workers were asked to evaluate the relationship between Natural Language Derivations (NLDs) and statements.",
    "evidence": [
      "s\nTable TABREF53 shows examples of crowdsourced annotations.\n\n\n",
      "::: Quality\nTo evaluate the quality of annotation results, we publish another CS task on AMT. We randomly sample 300 True and Likely responses in this evaluation. Given NLDs and a statement, 3 crowdworkers are asked if the NLDs can lead to the statement at four scale levels. If the answer is 4 or 3 ",
      "ized as follows: We create a large corpus consisting of 12,000 QA pairs and natural language derivations. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Through an experiment using two baseline models, we highlight several challenges o",
      "e corpus of NLDs. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Our experiments using two simple baseline models have demonstrated that RC-QED$^{\\rm E}$ is a non-trivial task, and that it indeed provides a challenging task of extracti",
      "te future work is to expand the annotation to non-entity-based multi-hop QA datasets such as HotpotQA BIBREF2. For modeling, we plan to incorporate a generative mechanism based on recent advances in conditional language modeling.\n\n\nExample annotations\nTable TABREF53 shows examples of crowdsourced an"
    ]
  },
  {
    "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* A data collection process was conducted to identify 400 authentic gang member profiles on Twitter.\n* The process involved using location-neutral keywords used by gang members, expanding the search to their retweet, friends, and follower networks.\n* A Twitter user dataset was created, consisting of 400 gang member and 2,865 non-gang member profiles.\n\nAccording to the context, the ground truth of gang membership is established through the manual curation of a set of gang member profiles, which may be biased towards a small geographic area.",
    "evidence": [
      "an be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our st",
      "fying gang member profiles on Twitter. Using a Twitter user dataset that consist of 400 gang member and 2,865 non gang member profiles, we trained word embedding models based on users' tweets, profile descriptions, emoji, images, and videos shared on Twitter (textual features extracted from images, ",
      "nguish between gang and non-gang member profiles. While a very promising INLINEFORM0 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for class",
      "l set of manually curated gang member profiles, often from a small geographic area that may bias their findings. In our previous work BIBREF9 , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the conten",
      "Abstract\nGang affiliates have joined the masses who use social media to share thoughts and actions publicly. Interestingly, they use this public medium to express recent illegal actions, to intimidate others, and to share outrageous images and statements. Agencies able to unearth these profiles may "
    ]
  },
  {
    "title": "Recurrently Controlled Recurrent Networks",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The RCRN architecture has approximately equal parameters to 3L-BiLSTM, which is another model used for comparison.\n* The RCRN architecture is approximately three times larger than 3L-BiLSTM.\n\nAnswer: According to the context, the RCRN architecture has approximately equal parameters to 3L-BiLSTM, but it is also approximately three times larger than 3L-BiLSTM.",
    "evidence": [
      "to stacked RNNs of approximately equal parameterization. There are several potential interesting directions for further investigating RCRNs. Firstly, investigating RCRNs controlling other RCRNs and secondly, investigating RCRNs in other domains where recurrent models are also prevalent for sequence ",
      "STM). This is to fairly observe the impact of different encoder models based on the same overall model framework.\n\n\nOverall Results\nThis section discusses the overall results of our experiments. On the 16 review datasets (Table TABREF22 ) from BIBREF32 , BIBREF31 , our proposed RCRN architecture ach",
      "hen we consider how two controller BiRNNs are used to control a single listener BiRNN. As such, for our experiments, when considering only the encoder and keeping all other components constant, 3L-BiLSTM has equal parameters to RCRN while RCRN and 3L-BiLSTM are approximately three times larger than ",
      " i.e., the listener. The listener uses a single BiRNN model for sequence encoding and then allows this representation to be altered by listening to the controller. An alternative interpretation to our model architecture is that it is essentially a `recurrent-over-recurrent' model. Clearly, the formu",
      "d LSTM-CNN, etc. We achieve (close to) state-of-the-art performance on SST, TREC question classification and 16 Amazon review datasets.\n\n\nRuntime Analysis\nThis section aims to get a benchmark on model performance with respect to model efficiency. In order to do that, we benchmark RCRN along with BiL"
    ]
  },
  {
    "title": "Contextual Recurrent Units for Cloze-style Reading Comprehension",
    "answer": "Based on the provided context, I summarize the key points related to the question as follows:\n\n* The CRU model is tested on two NLP tasks: sentiment classification and cloze-style reading comprehension.\n* The model is applied to these tasks without mentioning specific datasets.\n\nSince the context does not mention specific datasets used for testing sentiment classification and reading comprehension, I answer the question as follows:\n\nI cannot find the answer from the given context.",
    "evidence": [
      "ading comprehension task, we first present some modifications to a recent reading comprehension model, called AoA Reader BIBREF10, and then replace the GRU part by our CRU model to see if our model could give substantial improvements over strong baselines.\n\n\nApplications ::: Sentiment Classification",
      " tested our CRU model on sentence-level and document-level modeling NLP tasks: sentiment classification and reading comprehension. Experimental results show that the proposed CRU model could give significant improvements over traditional CNN or RNN models, including bidirectional conditions, as well",
      "be given in the next section.\n\n\nApplications ::: Reading Comprehension\nBesides the sentiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task. In this paper, we strengthened the recent AoA Reader BIBREF10 and applied our CRU mode",
      "ently modeling the sequence. We have tested our CRU model on the cloze-style reading comprehension task and sentiment classification task. Experimental results show that our model could give substantial improvements over various state-of-the-art systems and set up new records on the respective publi",
      " modeling tasks, in this paper, we applied the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some mo"
    ]
  },
  {
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "answer": "Based on the provided context, the key points are:\n\n* The authors compared the results of deleting important words versus unimportant words to see how it affects the summaries generated.\n* They observed that deleting important words leads to a quick degradation of the summaries, while deleting unimportant words does not have a significant impact.\n* The authors also compared the summaries generated with the target summaries and found that the generated summaries are not very accurate, with problems such as wrong reproduction of factual details, replacing rare words with more common alternatives, or repeating non-sense.\n\nAs for the question, the answer is: The authors compared the results of deleting important words versus unimportant words.",
    "evidence": [
      " are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the ",
      "on alternatives or repeating non-sense after the third sentence. We can see in Figure 1 an example of summary obtained compared to the target one. The \u201csummaries\" we generate are far from being valid summaries of the information in the texts but are sufficient to look at the attribution that LRP wil",
      " results of See et al. See2017. We obtain summaries that are broadly relevant to the text but do not match the target summaries very well. We observe the same problems such as wrong reproduction of factual details, replacing rare words with more common alternatives or repeating non-sense after the t",
      "truthful in regard to the network's computation and we may use it for further studies of the example, whereas for the text at the origin of Figure 5 we shouldn't draw any further conclusions from the attribution generated. One interesting point is that one saliency map didn't look \u201cbetter\" than the ",
      "ed it to the control experiment that consists of deleting the least important word and compare the degradation of the resulting summaries. We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words "
    ]
  },
  {
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "answer": "Based on the provided context, the answer to the question is:\n\nThey select answer candidates for their QA task by using the \"align, mask, and select\" (AMS) method. Specifically, they mask the pronoun word with a special token [QW] to construct a question, and then put the two candidate paragraphs as candidate answers.\n\nNote that the context does not mention any specific methods such as semi-supervised learning or Cross-View Training (CVT) for selecting answer candidates. The AMS method is the only approach mentioned in the context for selecting answer candidates.",
    "evidence": [
      "o a multi-choice question answering problem. We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks. We use the same loss function as BIBREF22 , that is, if c $_1$ is cor",
      " by the distant supervision approaches, to automatically construct a multi-choice question answering dataset. Third, Experiments demonstrate that the pre-trained model from the proposed approach with fine-tuning achieves significant performance improvements on several commonsense-related tasks, such",
      " QA task that chooses the correct answer from several candidate answers, we mask concept $_1$ and concept $_2$ and pre-train BERT with a masked language model (MLM) task. We denote the resulting model from this pre-training task BERT_MLM. We randomly mask 15% WordPiece tokens BIBREF27 of the questio",
      "ataset with the multi-choice question answering task and fine-tune it on various commonsense-related tasks, such as CommonsenseQA BIBREF10 and Winograd Schema Challenge (WSC) BIBREF13 , and achieve significant improvements. We also fine-tune and evaluate the pre-trained models on other NLP tasks, su",
      "matically by our proposed\"align, mask, and select\"(AMS) method. We also investigate different pre-training tasks. Experimental results demonstrate that pre-training models using the proposed approach followed by fine-tuning achieves significant improvements on various commonsense-related tasks, such"
    ]
  },
  {
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The context mentions two separate datasets built with sentences obtained from the Dutch Europarl corpus and the SoNaR corpus.\n* No specific sizes are mentioned for the datasets.\n\nAnswer: I cannot find the answer from the given context.",
    "evidence": [
      "ormance when trained and tested on the two corpora individually and experiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance. There are three settings: full in which the datasets contain full sentences, windowed in which sentenc",
      "n, the extra bidirectional LSTM layer, the influence of the second prediction task and/or the split in sentence and context encoder. Firstly, the data is divided into batch sizes of 512 instead of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batc",
      "tion and part-of-speech prediction of these two pronouns. Two separate datasets are built with sentences obtained from, respectively, the Dutch Europarl corpus (Koehn 2015) - which contains the proceedings of the European Parliament from 1996 to the present - and the SoNaR corpus (Oostdijk et al. 20",
      "tle consistent difference in performance when batch size is 512 or 128. Therefore, it can be suggested that an increased batch size has no directly positive influence on model performance. Secondly, the input data is transformed to 200-dimensional word embeddings instead of 100-dimensional word embe",
      " with windows exceeding sentence boundaries (3) outperforms that on the SoNaR dataset with windows within sentence boundaries (4) on every metric.\n\n\nMultitask Classification Model ::: Model Architecture\nThe second model performs two prediction tasks. The first prediction task remains the binary clas"
    ]
  },
  {
    "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors propose a dataset called [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m to facilitate research on developing solutions to challenging issues.\n* The dataset includes claims, perspectives, and evidence paragraphs, and the authors identify the following interdependent tasks as crucial for addressing the challenge:\n\t+ Determination of argue-worthy claims\n\t+ Identification of the key NLP challenges underlying this task\n\t+ Development of a dataset that allows a systematic study of these challenges\n\nThe challenges highlighted in the context are:\n\n* Determination of argue-worthy claims: not every claim requires an in-depth discussion of perspectives.\n* Identification of the key NLP challenges underlying the task of determining argue-worthy claims.\n\nThese challenges are addressed by the authors' proposal to develop a dataset that allows a systematic study of these challenges.",
    "evidence": [
      "the one studied here. To facilitate the research towards developing solutions to such challenging issues, we propose [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m, a dataset of claims, perspectives and evidence paragraphs. For a given ",
      "d, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each.\n\n\nRequired skills\nWe perform a closer investigation o",
      "NEFORM7 , denoted INLINEFORM8 . Creating systems that would address our challenge in its full glory requires solving the following interdependent tasks: Determination of argue-worthy claims: not every claim requires an in-depth discussion of perspectives. For a system to be practical, it needs to be",
      "s and their supporting evidence with respect to a given claim. Our goal is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges. For example, for the claim in Figure FIGREF1 , multiple (non-redundant) per",
      "ges and limitations will be addressed in future work.\n\n\nConclusion\nThe importance of this work is three-fold; we define the problem of substantiated perspective discovery and characterize language understanding tasks necessary to address this problem. We combine online resources, web data and crowds"
    ]
  },
  {
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The model is evaluated on a news comment dataset.\n* The dataset consists of 198,112 news articles, each with a title, article content, and a list of user comments.\n\nAccording to the context, the answer to the question is:\n\nThe news comment dataset used is not explicitly named, but it is described as a dataset consisting of 198,112 news articles with corresponding comments.",
    "evidence": [
      "ner. We evaluate our model on a news comment dataset. Experiments show that our proposed topic-based approach significantly outperforms previous lexicon-based models. The model also profits from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios.\n\n\nIntroduction\n",
      "n the Internet. To fully exploit the unpaired data, we completely remove the need for parallel data and propose a novel unsupervised approach to train an automatic article commenting model, relying on nothing but unpaired articles and comments. Our model is based on a retrieval-based commenting fram",
      "ews. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We coun",
      "dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in co",
      "odel is based on a retrieval-based commenting framework, which uses news to retrieve comments based on the similarity of their topics. The topic representation is obtained from a neural variational topic model, which is trained in an unsupervised manner. We evaluate our model on a news comment datas"
    ]
  },
  {
    "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The paper explores language mixing, specifically code-switching, in a multilingual community.\n* The experiment uses a conversational Mandarin-English code-switching speech corpus called SEAME Phase II.\n\nAccording to the context, the two languages explored in this paper are:\n\n* Mandarin\n* English",
    "evidence": [
      "\n\n\nIntroduction\nLanguage mixing has been a common phenomenon in multilingual communities. It is motivated in response to social factors as a way of communication in a multicultural society. From a sociolinguistic perspective, individuals do code-switching in order to construct an optimal interaction",
      "o an LSTM layer similar to BIBREF9 .\n\n\nCorpus\nIn our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by ",
      "um learning by training a network with monolingual corpora of both languages, and subsequently train on code-switched data. A further investigation of Equivalence Constraint and Curriculum Learning showed an improvement in language modeling BIBREF6 . A multi-task learning approach was introduced to ",
      "t using beam search with a beam size of 5. For the input, we build a parallel monolingual corpus by translating the mixed language sequence using Google NMT to English ( INLINEFORM0 ) and Mandarin ( INLINEFORM1 ) sequences. Then, we concatenate the translated English and Mandarin sequences and assig",
      "oose words from the monolingual sentences and form a grammatical code-switching sentence. In our experiment, we show that by training a language model using the augmented sentences we improve the perplexity score by 10% compared to the LSTM baseline.\n\n\nIntroduction\nLanguage mixing has been a common "
    ]
  },
  {
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "answer": "According to the context, the language identities are obtained by setting the language IDs $<chn>$ and $<eng>$ as shown in Fig. 1, and using the language identities to bias the model to predict the CS points. This allows the model to learn the language identity information from the transcription.",
    "evidence": [
      "ut symbols set with language IDs $<chn>$ and $<eng>$ as shown in Fig. 1, i.e., $\\hat{\\mathcal {Y}} \\in \\bar{\\mathcal {Y}} \\cup \\lbrace <chn>,<eng>\\rbrace $. The intuition behind it is that the CS in the transcript may obey a certain probability distribution, and this distribution can be learned by n",
      "tes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the proposed method can achieve 16.2% and 12.9% relative error reduction on two",
      "Abstract\nRecently, language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this work, we ",
      "which causes the system complex. In this work, we propose an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity infor",
      " language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our proposed method outperforms the RNN-T baseline ("
    ]
  },
  {
    "title": "Fully Automated Fact Checking Using External Sources",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The task-specific embeddings are built using the final hidden layer of the neural network.\n* The neural network is trained using Long Short-Term Memory networks (LSTMs) and is used to encode a claim together with pieces of potentially-relevant text fragments from the Web.\n* The Web evidence is taken into account when building the task-specific embeddings.\n\nAccording to the context, the data used to build the task-specific embeddings is:\n\n* The claim\n* Pieces of potentially-relevant text fragments from the Web\n* The source reliability of the Web evidence\n\nThese data are used to train the neural network, which is then used to build the task-specific embeddings.",
    "evidence": [
      " The features we use are dense representations of the claim, of the snippets and of related sentences from the Web pages, which we automatically train for the task using Long Short-Term Memory networks (LSTMs). We also use the final hidden layer of the neural network as a task-specific embedding of ",
      " with task-specific embeddings that encode a claim together with pieces of potentially-relevant text fragments from the Web, taking the source reliability into account. The evaluation results show good performance on two different tasks and datasets: (i) rumor detection and (ii) fact checking of the",
      "he neural network as a task-specific embedding of the claim, together with the Web evidence. We feed all these representations as features, together with pairwise similarities, into a Support Vector Machine (SVM) classifier using an RBF kernel to classify the claim as True or False. Figure FIGREF1 p",
      "using a bi-LSTM. Finally, we combine the SVM with the NN by augmenting the input to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this t",
      "pet, or (b) between the claim and a Web page: (i) cosine with tf-idf, (ii) cosine over embeddings, and (iii) containment BIBREF1 . We calculate the embedding of a text as the average of the embeddings of its words; for this, we use pre-trained embeddings from GloVe BIBREF2 . Moreover, as a Web page "
    ]
  },
  {
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "answer": "According to the context, the proposed framework outperforms state-of-the-art methods by about 1% absolute gains on each dataset when compared to \"OURS w/o THA\", and further improves the performance by adding THA, surpassing all state-of-the-art methods.",
    "evidence": [
      " shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. \u201cOURS w/o THA & STN\u201d only keeps the basic bi-linear attention. Although it performs not bad, it is still less co",
      "arsing, instead, they employ attention mechanism to distill opinion information to help aspect extraction. Our framework consistently performs better than them. The gains presumably come from two perspectives: (1) In our model, the opinion summary is exploited after performing the selective transfor",
      "FORM0  Then, the losses from both tasks are combined to form the training objective of the entire model: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 represent the loss functions for aspect and opinion extractions respectively.\n\n\nDatasets\nTo evaluate the effectiveness of the proposed framework fo",
      " on the training data of INLINEFORM0 , other hyper-parameters are set as follows: INLINEFORM1 , INLINEFORM2 ; the number of cached historical aspect representations INLINEFORM3 is 5; the learning rate of SGD is 0.07.\n\n\nMain Results\nAs shown in Table TABREF39 , the proposed framework consistently obt",
      "near attention, i.e. \u201cOURS w/o THA\u201d, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. \u201cOURS\u201d, the performance is further improved, and all state-of-the-art methods are surpassed.\n\n\nAttention Visualization and Case Study\nIn Figure FIG"
    ]
  },
  {
    "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The work evaluates the quality of sentence representations on several transfer tasks, including movie review sentiment (MR), customer reviews (CR), subjectivity (SUBJ), and opinion polarity (OP).\n* The datasets used for evaluation are not explicitly mentioned, but the context mentions the use of Karpathy and Fei-Fei's split for the MS-COCO dataset.\n* The O5K dataset is also mentioned, but its specific use is not clear.\n\nBased on the summarized information, the answer to the question is:\n\nThe work is evaluated over the MS-COCO dataset, using Karpathy and Fei-Fei's split, and possibly the O5K dataset as well.",
    "evidence": [
      "d from remaining data samples in the same mini-batch.\n\n\nEvaluation\nAdhering to the experimental settings of BIBREF1 , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBR",
      "model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) BIBREF32 , customer reviews (CR) BIBREF33 , subjectivity (SUBJ) BIBREF34 , opinion p",
      "Image features are prepared by extracting hidden representations at the final layer of ResNet-101 BIBREF30 . We evaluate sentence representation quality using SentEval BIBREF7 , BIBREF1 scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-bat",
      "O5K dataset BIBREF2 . We augment the state-of-the-art sentence representations with those produced by our model and conduct a series of experiments on transfer tasks to test the quality of sentence representations. Through detailed analysis, we confirm our hypothesis that self-attention help our mod",
      "nd number of attentions ( INLINEFORM4 ) is 30. We employ orthogonal initialization BIBREF28 for recurrent weights and xavier initialization BIBREF29 for all others. For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10 . Image features are prepared by extracting hidden r"
    ]
  },
  {
    "title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction",
    "answer": "Based on the provided context, the variables in the ablation study were:\n\n* The number of task-specific BiRNN layers in the architecture\n* The use of zero NER-task-specific layers\n\nThe ablation study was conducted to demonstrate the effectiveness of the additional task-specific BiRNN layers in the architecture.",
    "evidence": [
      "ters consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table TABREF26 contains the results from the ablation study. These results show that the proposed architecture benefits f",
      "trate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-",
      "ed than they are in the case of the CoNLL04 dataset. Intuitively, cases in which the NER and RE problems can be solved by relying on more shared information should require fewer task-specific layers.\n\n\nExperiments ::: Ablation Study\nTo further demonstrate the effectiveness of the additional task-spe",
      " achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggest",
      "i$, $\\mathbf {s}^{e}_i$, is obtained by passing $\\mathbf {h}^{e}_i$ through a series of two feed forward layers: The activation function of $\\text{FFNN}^{(e1)}$ and its output size are treated as hyperparameters. $\\text{FFNN}^{(e2)}$ uses linear activation and its output size is $|\\mathcal {E}|$, wh"
    ]
  },
  {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors evaluate their model on two standard datasets for named entity recognition (NER) and chunking.\n* The datasets used are not explicitly mentioned by name, but they are referred to as \"two standard datasets\" and are used for NER and chunking tasks.\n\nAccording to the context, the evaluation datasets are:\n\n* CoNLL 2000 chunks\n* PTB POS tags\n\nNote that the context does not mention the names of the datasets, but it does mention the tasks and the sources of the data.",
    "evidence": [
      " CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word emb",
      "d slightly to INLINEFORM4 indicating that the additional parameters in TagLM are slightly hurting performance. One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional L",
      "revious state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available. Furthermore, Tables TABREF17 and TABREF18 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained",
      "sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific g",
      "g data and transferring from both CoNLL 2000 chunks and PTB POS tags. Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large. On the other hand, our appr"
    ]
  },
  {
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The performance of the models starts to increase at around 10 million finetuned sentences.\n* The standard deviation in accuracy amounts to about 1% for all experiments, justifying the need to average over 9 runs to measure differences in model performance reliably.\n* The in-domain training case results are shown in Table:results.\n\nBased on this summary, the answer to the question is:\n\nThe performance results show that the models start to increase in performance at around 10 million finetuned sentences, with a standard deviation in accuracy of about 1%.",
    "evidence": [
      "he laptops domain the performance starts to increase at about 10 Mio. finetuned sentences. This is an interesting insight as one would expect a relation closer to a logarithmic curve. One reason might be that it takes many steps to train knowledge into the BERT language model due to its vast amount ",
      "domain-specific finetuning could probably bring significant performance improvements. Another interesting direction for future work would be to investigate cross-domain behavior for an additional domain like hotels, which is more similar to the restaurants domain. Here, it could be interesting to fi",
      "tandard deviation amounts to about $1\\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably. To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt an",
      "om initializations. This is needed to measure significance of improvements, as the standard deviation in accuray amounts to roughly $1\\%$ for all experiments, see fig:acc-dep-lmiterations.\n\n\nExperiments ::: Compared Methods\nWe compare in-domain results to current state of the art methods, which we w",
      "ing, we removed reviews containing less than two sentences. For the laptop corpus, $1,007,209$ sentences are left after pre-processing. For the restaurants domain more reviews are available, we sampled $10,000,000$ sentences to have a sufficient amount of data for fully exploitet language model fine"
    ]
  },
  {
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\nThe paper does not explicitly mention the existing models they compared with. However, it mentions that most existing MRC models are focusing on dealing with the first three types of questions (i.e., (1) Natural Language Inference; (2) Coreference Resolution; (3) Logic-based reasoning), but they suffer from problems when answering questions requiring numerical reasoning.\n\nAnswer: I cannot find the answer from the given context.",
    "evidence": [
      "e enhancements are also used in our NumNet model and the details are given in the Appendix.\n\n\nExperiments ::: Experimental Settings\nIn this paper, we tune our model on the development set and use a grid search to determine the optimal parameters. The dimensions of all the representations (e.g., $\\ma",
      " $9,622$ testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following BIBREF6. The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric go",
      "ogic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best ",
      "the comparing questions which contain answer candidates, we observe that the workers frequently only change the incorrect answer candidate to generate a new question. For example, \u201cHow many from the census is bigger: Germans or English?\u201d whose golden answer is \u201cGermans\u201d is modified to \u201cHow many from",
      " a random sample of NumNet predictions. We find that: (1) Our NumNet model can answer about 76% of sorting/comparison questions correctly, which indicates that our NumNet model has achieved numerical reasoning ability to some extend. (2) Among the incorrectly answered sorting/comparison questions, t"
    ]
  },
  {
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context does not explicitly mention a specific language being looked at.\n* However, it mentions \"cross-lingual\" topic classification and retrieval, which implies that the study is considering multiple languages.\n* The context also mentions \"high-resource\" and \"low-resource\" languages, but does not specify which languages fall into these categories.\n\nBased on this summary, I would answer the question as follows:\n\nThe context does not specify a particular language being looked at, but it does mention the consideration of cross-lingual topic classification and retrieval, which implies that multiple languages are being considered.",
    "evidence": [
      "recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic classification for speech has not been considered elsewhere, as far as we know.\n\n\nConclusions and future work\nOur results show that poor speech translatio",
      "ll these studies, the query and search languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a c",
      "Abstract\nGiven a large amount of unannotated speech in a language with few resources, can we classify the speech utterances by topic? We show that this is possible if text translations are available for just a small amount of speech (less than 20 hours), using a recent model for direct speech-to-tex",
      "classify 61% of the speech segments. Cross-lingual topic modeling may be useful when the target language is high-resource. Here, we learned target topics just from the 20 hours of translations, but in future work, we could use a larger text corpus in the high-resource language to learn a more genera",
      "ds appears in each segment. Recent studies have extended the early work to end-to-end keyword spotting BIBREF18, BIBREF19 and to semantic keyword retrieval, where non-exact but relevant keyword matches are retrieved BIBREF20, BIBREF21, BIBREF22. In all these studies, the query and search languages a"
    ]
  },
  {
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The LAXARY model is developed to provide trustworthy PTSD assessment among clinicians.\n* Clinically validated survey tools are used to build category sets based on survey scores.\n* These survey tools are used to detect the prevalence of PTSD and obtain trust from clinicians.\n\nAccording to the context, the clinically validated survey tools used are not explicitly mentioned. However, it is mentioned that \"there are many clinically validated PTSD assessment tools\" that are being used.\n\nAnswer: The context does not specify the exact clinically validated survey tools used, but it mentions that there are many such tools being used.",
    "evidence": [
      "ils to provide trust of clinicians due to its lack of interpretability in clinical terms. In this paper, we develop LAXARY model where first we start investigating clinically validated survey tools which are trustworthy methods of PTSD assessment among clinicians, build our category sets based on th",
      "urvey scores (similar to sentiment category scores of naive LIWC) which is both explainable and trustworthy to clinicians.\n\n\nDemographics of Clinically Validated PTSD Assessment Tools\nThere are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD ",
      "ng clinicians, build our category sets based on the survey questions and use these as dictionary words in terms of first person singular number pronouns aspect for next level LIWC algorithm. Finally, we develop a modified LIWC algorithm to estimate survey scores (similar to sentiment category scores",
      "ealth assessment framework of any kind.\n\n\n",
      "out incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment. Finally, we evaluate the accuracy of LAXARY model performance and reliability-validity of generated PTSD Li"
    ]
  },
  {
    "title": "Reference-less Quality Estimation of Text Simplification Systems",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\nThe paper compares multiple approaches to reference-less quality estimation of sentence-level text simplification (TS). The approaches are compared in a reference-less setting, analyzing the results.\n\nAccording to the context, the approaches compared include:\n\n* Using MT metrics to compare the output and a reference\n* Using MT metrics to compare the output and the source sentence\n\nThese approaches are compared to evaluate their effectiveness in measuring the quality of TS systems, including meaning preservation, grammaticality, and simplicity.\n\nAnswer: The approaches compared are using MT metrics to compare the output and a reference, and using MT metrics to compare the output and the source sentence.",
    "evidence": [
      "hey are still very superficial features and might miss some deeper and more complex information. Moreover the fact that grammaticality and meaning preservation are best evaluated using INLINEFORM1 -gram-based comparison metrics might bias the TS models towards copying the source sentence and applyin",
      "y we compare all methods in a reference-less setting and analyze the results.\n\n\nUsing MT metrics to compare the output and a reference\nTS can be considered as a monolingual translation task. As a result, MT metrics such as BLEU BIBREF15 , which compare the output of an MT system to a reference trans",
      " evaluate these metrics with respect to meaning preservation and grammaticality. We extend their work in two directions. Firstly, we extend the comparison to include the degree of simplicity achieved by the system. Secondly, we compare additional features, including those used by BIBREF18 , both ind",
      "nce data, which is rarely available for TS. TS has the advantage over MT of being a monolingual task, which allows for direct comparisons to be made between the simplified text and its original version. In this paper, we compare multiple approaches to reference-less quality estimation of sentence-le",
      "a good way to perform TS evaluation, even when good quality references are available. This is especially true when the TS system produces more than one sentence for a single source sentence.\n\n\nUsing MT metrics to compare the output and the source sentence\nAs mentioned in the Introduction, the fact t"
    ]
  },
  {
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The paper evaluates its approach on three Twitter sentiment classification datasets.\n* The datasets used are not explicitly mentioned in the context, but we can infer that they are the Stanford Twitter Sentiment Corpus, Sanders - Twitter Sentiment Corpus, and possibly another dataset (as the paper mentions \"three Twitter sentiment classification datasets\").\n\nAnswer: The three Twitter sentiment classification datasets used for experiments are the Stanford Twitter Sentiment Corpus, Sanders - Twitter Sentiment Corpus, and possibly another dataset (not explicitly mentioned).",
    "evidence": [
      "e evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.\n\n\nIntroduction\nTwitter sentiment classification have intensively researched ",
      "timent classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB",
      " the weight vectors BIBREF18 .\n\n\n Experimental setups\nFor the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a bin",
      "177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, ",
      "t of pre-train words are initialized randomly.\n\n\nExperimental results\nTable IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxE"
    ]
  },
  {
    "title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\n* The context mentions the evaluation of the performance of different systems on a specific task, which is the task of discovering latent user attributes disclosed through text.\n* The evaluation metrics used are not explicitly mentioned in the context, but it does mention the measurement of inter-annotator agreement via Fleiss kappa.\n\nAs for the answer to the question, I cannot find the specific evaluation metrics used in the context. The context only mentions Fleiss kappa as a measure of inter-annotator agreement, but it does not provide information on the metrics used to evaluate the performance of the systems.",
    "evidence": [
      "we had six judges, balanced for gender, and obtained three annotations per target user. Inter-annotator agreement for the tasks was measured via Fleiss kappa ( INLINEFORM0 ), and was higher for the in-language experiment ( INLINEFORM1 ) than for the cross-language tasks (NL INLINEFORM2 PT: INLINEFOR",
      "cross-language tasks (NL INLINEFORM2 PT: INLINEFORM3 ; FR INLINEFORM4 NL: INLINEFORM5 ). Table TABREF22 shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users. Systems were tested under tw",
      "nally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine. If humans can predict gende",
      "pt the features from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual embeddings model we use the mean embedding representation from the system of BIBREF27 and add max, std and coverage features. We create multil",
      " is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . It is of interest to several applications including personalized machine translation, for"
    ]
  },
  {
    "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The study aims to predict demographic attributes using Yahoo! Answers and Twitter data.\n* The task is defined as a regression task to predict a continuous-valued demographic attribute for unseen neighbourhoods.\n* The study focuses on a wide range of 62 demographic attributes.\n\nAccording to the context, the answer to the question is:\n\nThey try to predict 62 demographic attributes.",
    "evidence": [
      "estigate how well the demographic attributes can be predicted by using using Yahoo! Ansewrs and Twitter data. We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their normalised tf-idf document representation. A separate re",
      "idely used for predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from bo",
      "show the prediction results for a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we display two terms with the highest coefficient common between the majority of the folds. Attributes are divided into categories such as Religion, Employment, Education, ",
      "ssions and the observed demographic attributes can reach an average Pearson correlation coefficient of \\r{ho} = 0.54, slightly higher than the predictions obtained using Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted",
      "nally to its size. The results of the regression tasks performed over the selected set of demographic attributes, in terms of Pearson correlation coefficient ( $\\rho $ ), are presented in Table 4 . Results are averaged over 10 folds and standard deviations are displayed in parenthesis. We can see th"
    ]
  },
  {
    "title": "Bias in Semantic and Discourse Interpretation",
    "answer": "Based on the provided context, here is a summary of the key points related to the question:\n\nThe paper discusses interpretive bias, which is an important topic with significant social implications. The authors analyze interpretive bias in the context of discourse structure and its impact on accurately extracting sentiments and opinions from text. The paper relies on three key premises, including the idea that histories are discursive interpretations of a set of data.\n\nAs for the question, the answer is:\n\nThe paper analyzes interpretive bias in general, without specifying particular types of biases. It does not focus on a specific type of interpretive bias, but rather provides a general analysis of the concept and its implications.\n\nNote that the context does not mention specific methods for analyzing interpretive bias, such as semi-supervised learning or Cross-View Training (CVT).",
    "evidence": [
      "ions in Section SECREF4 . A detailed and formal analysis of interpretive bias has important social implications. Questions of bias are not only timely but also pressing for democracies that are having a difficult time dealing with campaigns of disinformation and a society whose information sources a",
      "cts deviate from prescriptively rational or independently given objective outcomes in quantitative decision making and frequency estimation, even though they arguably have the goal of seeking an optimal or \u201ctrue\u201d solution. In a general analysis of interpretive bias like ours, however, it is an open ",
      "ortant discussion, which is in turn buttressed by the reporter's mentioning of the responses of the Times' readership. A formally precise account of interpretive bias will thus require an analysis of histories and their structure and to this end, we exploit Segmented Discourse Representation Theory ",
      "en shown in the literature that discourse structure is an important factor in accurately extracting sentiments and opinions from text BIBREF15 , BIBREF16 , BIBREF17 , and our examples show that this is the case for interpretive bias as well.\n\n\nEpistemic ME games\nThe second ingredient needed to under",
      "if so, under what conditions. Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that histories are discursive interpretations of a set of data in the sense that like dis"
    ]
  },
  {
    "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* The authors compare three types of word segmentation methods.\n* One of the methods used is an unsupervised word segmentation method called \"latticelm\" that can directly segment words from the lattices of the speech recognition results.\n\nAccording to the context, the method used for word segmentation is \"latticelm\", which is an unsupervised word segmentation method.",
    "evidence": [
      "e compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences INLINEFORM0 . The remainder of this section is organized as fol",
      "n words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more promisi",
      "of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the sylla",
      " taught for the same place, the word whose PAR was the higher score was adopted. Fig. FIGREF63 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition resul",
      "y better than DPM, a word clustering method without location information, irrespective of the word segmentation method used. The experimental results showed that it is possible to improve the estimation accuracy of spatial concepts and vocabulary by performing word clustering that considered locatio"
    ]
  },
  {
    "title": "Localization of Fake News Detection via Multitask Transfer Learning",
    "answer": "Based on the provided context, I summarize the key points related to the question:\n\n* The context mentions two datasets: the fake news dataset and the LIAR dataset.\n* The fake news dataset has 972 labeled stances for each headline-body pairs.\n* The LIAR dataset contains 12,836 labeled short statements as well as sources to support the labels.\n\nAccording to the summarized information, the size of the dataset is not a single value, but rather two different sizes:\n\n* The fake news dataset has 972 labeled stances for each headline-body pairs.\n* The LIAR dataset has 12,836 labeled short statements.\n\nTherefore, the answer to the question is: The size of the dataset is 972 (for the fake news dataset) and 12,836 (for the LIAR dataset).",
    "evidence": [
      " size of 32, and a learning rate of 3e-5. For ULMFiT, we perform language model finetuning on the fake news dataset (appending no extra classification heads yet) for a total of 10 epochs, using a learning rate of 1e-2, a batch size of 80, and weight decay of 0.3. For the final ULMFiT finetuning stag",
      "modeling as its sole pretraining task, according to the specifications of BIBREF8. We use an embedding dimension of 410, a hidden dimension of 2100, and a maximum sequence length of 256. We use 10 attention heads per multihead attention block, with 16 blocks composing the encoder of the transformer.",
      " 32, and a learning rate of 2e-5. For GPT-2, our classification head is first comprised of a layer normalization transform, followed by a linear layer, then a softmax transform. We finetune the pretrained GPT-2 transformer for 3 epochs, using a batch size of 32, and a learning rate of 3e-5. For ULMF",
      ",972 labeled stances for each headline-body pairs. BIBREF2, on the other hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels. This requirement for large datasets to effectively train fake news detection models from scratch make",
      "from a mechanism called \u201cAttention\u201d BIBREF16, which allows the model to compute weighted importance for each token in a sequence, effectively pinpointing context reference BIBREF15. Precisely, we compute attention on a set of queries packed as a matrix $Q$ on key and value matrices $K$ and $V$, resp"
    ]
  },
  {
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "answer": "Based on the provided context, the key points related to the question are:\n\n* Using less than 50% of the available training examples for each vocabulary size might result in overfitting.\n* The validation loss consistently increases after about 15 epochs when using less than 50% of the data.\n* The trend is that by using less than 50% of the data, the model tends to overfit the data.\n\nThe experimental results that suggest using less than 50% of the available training examples might result in overfitting are:\n\n* The validation loss consistently increases after about 15 epochs when using less than 50% of the data.\n* The model tends to overfit the data when using less than 50% of the data.\n\nTherefore, the answer is: The experimental results that suggest using less than 50% of the available training examples might result in overfitting are the consistent increase in the validation loss after about 15 epochs and the trend of overfitting when using less than 50% of the data.",
    "evidence": [
      "w that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relat",
      "s that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of ",
      " trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drasti",
      "lidation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words",
      "tion) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data "
    ]
  }
]